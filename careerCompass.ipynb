{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09529a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ID = \"alek4h7jlreffeoe5tocxgnx2u\"\n",
    "CAREER_SALARY_TABLE      = f\"careerSalary-{API_ID}-NONE\"\n",
    "CAREER_EDUCATION_TABLE   = f\"careerEducation-{API_ID}-NONE\"\n",
    "CAREER_SKILLS_TABLE      = f\"careerSkills-{API_ID}-NONE\"\n",
    "CAREER_DESCRIPTION_TABLE = f\"careerDescription-{API_ID}-NONE\"\n",
    "SOC_CODES_TABLE          = f\"socCodes-{API_ID}-NONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting upload from all CSVs in: Datasets/dynamodb_ready_by_year\n",
      "📂 Queuing data from oes_research_2016_allsectors_dynamodb_ready.csv\n",
      "✅ Batch 1 inserted in 4.06s (1000 records)\n",
      "✅ Batch 1 inserted in 4.30s (1000 records)\n",
      "✅ Batch 1 inserted in 4.40s (1000 records)\n",
      "✅ Batch 1 inserted in 4.48s (1000 records)\n",
      "✅ Batch 2 inserted in 3.35s (1000 records)\n",
      "✅ Batch 2 inserted in 3.43s (1000 records)\n",
      "✅ Batch 2 inserted in 3.65s (1000 records)\n",
      "✅ Batch 2 inserted in 3.99s (1000 records)\n",
      "✅ Batch 3 inserted in 3.31s (1000 records)\n",
      "✅ Batch 3 inserted in 3.66s (1000 records)\n",
      "✅ Batch 3 inserted in 3.19s (1000 records)\n",
      "✅ Batch 3 inserted in 3.83s (1000 records)\n",
      "📂 Queuing data from oes_research_2017_allsectors_dynamodb_ready.csv\n",
      "✅ Batch 4 inserted in 2.68s (1000 records)\n",
      "✅ Batch 4 inserted in 3.09s (1000 records)\n",
      "✅ Batch 4 inserted in 2.91s (1000 records)\n",
      "✅ Batch 4 inserted in 3.66s (1000 records)\n",
      "✅ Batch 5 inserted in 3.29s (1000 records)\n",
      "✅ Batch 5 inserted in 3.81s (1000 records)\n",
      "✅ Batch 5 inserted in 4.76s (1000 records)\n",
      "✅ Batch 5 inserted in 4.19s (1000 records)\n",
      "✅ Batch 6 inserted in 3.37s (1000 records)\n",
      "✅ Batch 6 inserted in 2.98s (1000 records)\n",
      "✅ Batch 6 inserted in 2.61s (1000 records)\n",
      "✅ Batch 6 inserted in 2.48s (1000 records)\n",
      "✅ Batch 7 inserted in 2.44s (1000 records)\n",
      "✅ Batch 7 inserted in 2.30s (1000 records)\n",
      "✅ Batch 7 inserted in 2.11s (1000 records)\n",
      "✅ Batch 7 inserted in 2.10s (1000 records)\n",
      "✅ Batch 8 inserted in 1.89s (1000 records)\n",
      "✅ Batch 8 inserted in 2.50s (1000 records)\n",
      "✅ Batch 8 inserted in 2.42s (1000 records)\n",
      "✅ Batch 8 inserted in 2.50s (1000 records)\n",
      "✅ Batch 9 inserted in 3.14s (1000 records)\n",
      "✅ Batch 9 inserted in 2.99s (1000 records)\n",
      "✅ Batch 9 inserted in 2.69s (1000 records)\n",
      "✅ Batch 9 inserted in 2.81s (1000 records)\n",
      "✅ Batch 10 inserted in 1.94s (1000 records)\n",
      "✅ Batch 10 inserted in 2.99s (1000 records)\n",
      "✅ Batch 10 inserted in 2.85s (1000 records)\n",
      "✅ Batch 10 inserted in 3.00s (1000 records)\n",
      "✅ Batch 11 inserted in 2.91s (1000 records)\n",
      "📂 Queuing data from oes_research_2018_allsectors_dynamodb_ready.csv\n",
      "✅ Batch 11 inserted in 3.51s (1000 records)\n",
      "✅ Batch 11 inserted in 3.64s (1000 records)\n",
      "✅ Batch 11 inserted in 3.56s (1000 records)\n",
      "✅ Batch 12 inserted in 3.58s (1000 records)\n",
      "✅ Batch 13 inserted in 2.43s (1000 records)\n",
      "✅ Batch 12 inserted in 3.02s (1000 records)\n",
      "✅ Batch 12 inserted in 3.08s (1000 records)\n",
      "✅ Batch 12 inserted in 3.20s (1000 records)\n",
      "✅ Batch 14 inserted in 2.72s (1000 records)\n",
      "✅ Batch 13 inserted in 2.70s (1000 records)✅ Batch 13 inserted in 2.55s (1000 records)\n",
      "\n",
      "✅ Batch 13 inserted in 2.61s (1000 records)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    112\u001b[39m         filepath = os.path.join(CLEANED_DIR, filename)\n\u001b[32m    113\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m📂 Queuing data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[43mread_and_queue_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m queue.join()\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Stop workers\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mread_and_queue_csv\u001b[39m\u001b[34m(filepath)\u001b[39m\n\u001b[32m     63\u001b[39m reader = csv.DictReader(f)\n\u001b[32m     64\u001b[39m batch = []\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnow\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_timestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\trupti\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\csv.py:178\u001b[39m, in \u001b[36mDictReader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.line_num == \u001b[32m0\u001b[39m:\n\u001b[32m    176\u001b[39m     \u001b[38;5;66;03m# Used only for its side effect.\u001b[39;00m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28mself\u001b[39m.fieldnames\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m row = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28mself\u001b[39m.line_num = \u001b[38;5;28mself\u001b[39m.reader.line_num\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# unlike the basic reader, we prefer not to return blanks,\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# because we will typically wind up with a dict full of None\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# values\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:322\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch 15 inserted in 1.62s (1000 records)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch 14 inserted in 1.67s (1000 records)\n",
      "✅ Batch 14 inserted in 1.68s (1000 records)\n",
      "✅ Batch 14 inserted in 1.55s (1000 records)\n",
      "✅ Batch 16 inserted in 1.75s (1000 records)\n",
      "✅ Batch 15 inserted in 1.73s (1000 records)\n",
      "✅ Batch 15 inserted in 1.88s (1000 records)\n",
      "✅ Batch 15 inserted in 1.77s (1000 records)\n",
      "✅ Batch 17 inserted in 1.68s (1000 records)\n",
      "✅ Batch 16 inserted in 1.65s (1000 records)\n",
      "✅ Batch 16 inserted in 1.72s (1000 records)\n",
      "✅ Batch 16 inserted in 1.76s (1000 records)\n",
      "✅ Batch 18 inserted in 1.55s (1000 records)\n",
      "✅ Batch 17 inserted in 1.62s (1000 records)\n",
      "✅ Batch 17 inserted in 1.54s (1000 records)\n",
      "✅ Batch 17 inserted in 1.64s (1000 records)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import threading\n",
    "from decimal import Decimal\n",
    "from datetime import datetime, timezone\n",
    "import boto3\n",
    "from queue import Queue\n",
    "\n",
    "# Config\n",
    "CLEANED_DIR = \"Datasets/dynamodb_ready_by_year\"\n",
    "REGION = \"us-west-2\"\n",
    "BATCH_SIZE = 1000\n",
    "NUM_THREADS = 4\n",
    "REJECTED_FILE = \"rejected_dynamodb_rows.csv\"\n",
    "\n",
    "# AWS setup\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=REGION)\n",
    "table = dynamodb.Table(CAREER_SALARY_TABLE)\n",
    "\n",
    "queue = Queue()\n",
    "rejected_rows = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def to_decimal(val):\n",
    "    try:\n",
    "        return Decimal(str(round(float(val), 2))) if val not in [None, '', 'null'] else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def current_timestamp():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def process_batch(batch, batch_number):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        with table.batch_writer(overwrite_by_pkeys=['occCode']) as writer:\n",
    "            for item in batch:\n",
    "                try:\n",
    "                    writer.put_item(Item=item)\n",
    "                except Exception as e:\n",
    "                    with lock:\n",
    "                        rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"✅ Batch {batch_number} inserted in {time.time() - start:.2f}s ({len(batch)} records)\")\n",
    "    except Exception as e:\n",
    "        with lock:\n",
    "            for item in batch:\n",
    "                rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"❌ Batch {batch_number} failed with error: {str(e)}\")\n",
    "\n",
    "def worker():\n",
    "    batch_number = 1\n",
    "    while True:\n",
    "        batch = queue.get()\n",
    "        if batch is None:\n",
    "            break\n",
    "        process_batch(batch, batch_number)\n",
    "        batch_number += 1\n",
    "        queue.task_done()\n",
    "\n",
    "def read_and_queue_csv(filepath):\n",
    "    with open(filepath, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        batch = []\n",
    "        for row in reader:\n",
    "            try:\n",
    "                now = current_timestamp()\n",
    "                item = {\n",
    "                    'occCode': str(row['occ_code']),\n",
    "                    'salaryKey': str(row['salary_key']),\n",
    "                    'aMedian': to_decimal(row.get('a_median')),\n",
    "                    'mMedian': to_decimal(row.get('m_median')),\n",
    "                    'mPct10': to_decimal(row.get('m_pct10')),\n",
    "                    'mPct90': to_decimal(row.get('m_pct90')),\n",
    "                    'createdAt': now,\n",
    "                    'updatedAt': now,\n",
    "                }\n",
    "                # Remove None fields\n",
    "                item = {k: v for k, v in item.items() if v is not None}\n",
    "                batch.append(item)\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "                    queue.put(batch)\n",
    "                    batch = []\n",
    "            except Exception as e:\n",
    "                with lock:\n",
    "                    rejected_rows.append({**row, 'error': str(e)})\n",
    "\n",
    "        if batch:\n",
    "            queue.put(batch)\n",
    "\n",
    "def save_rejected_rows():\n",
    "    if rejected_rows:\n",
    "        keys = rejected_rows[0].keys()\n",
    "        with open(REJECTED_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rejected_rows)\n",
    "        print(f\"⚠️ {len(rejected_rows)} records rejected. Saved to {REJECTED_FILE}\")\n",
    "\n",
    "# Main\n",
    "print(f\"🚀 Starting upload from all CSVs in: {CLEANED_DIR}\")\n",
    "start = time.time()\n",
    "\n",
    "threads = []\n",
    "for _ in range(NUM_THREADS):\n",
    "    t = threading.Thread(target=worker)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "for filename in os.listdir(CLEANED_DIR):\n",
    "    if filename.endswith(\"_dynamodb_ready.csv\"):\n",
    "        filepath = os.path.join(CLEANED_DIR, filename)\n",
    "        print(f\"📂 Queuing data from {filename}\")\n",
    "        read_and_queue_csv(filepath)\n",
    "\n",
    "queue.join()\n",
    "\n",
    "# Stop workers\n",
    "for _ in range(NUM_THREADS):\n",
    "    queue.put(None)\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "save_rejected_rows()\n",
    "print(f\"🎉 All batches uploaded in {time.time() - start:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc8c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Reading and pivoting CSV: Datasets/education_data.csv\n",
      "📦 Total DynamoDB items to insert: 175\n",
      "🚀 Starting single-threaded DynamoDB insert...\n",
      "✅ Batch 1 (25) inserted [1-25]\n",
      "✅ Batch 2 (25) inserted [26-50]\n",
      "✅ Batch 3 (25) inserted [51-75]\n",
      "✅ Batch 4 (25) inserted [76-100]\n",
      "✅ Batch 5 (25) inserted [101-125]\n",
      "✅ Batch 6 (25) inserted [126-150]\n",
      "✅ Batch 7 (25) inserted [151-175]\n",
      "Total items inserted: 175\n",
      "⏱️ Single-threaded insert time: 1.05 sec\n",
      "🚀 Starting multi-threaded DynamoDB insert (4 threads)...\n",
      "✅ Batch 2 (100) inserted\n",
      "✅ Batch 2 (75) inserted\n",
      "Total items inserted: 175\n",
      "⏱️ Multi-threaded insert time: 0.92 sec\n",
      "🎯 Script completed.\n"
     ]
    }
   ],
   "source": [
    "#insert education data\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timezone\n",
    "import boto3\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Input CSV and table config\n",
    "input_file = \"Datasets/education_data.csv\"\n",
    "region = \"us-west-2\"\n",
    "\n",
    "# Updated field mapping to match new schema (camelCase)\n",
    "CODE_TO_FIELD = {\n",
    "    'Less_than_hs': 'lessThanHs',\n",
    "    'hs_or_eq': 'hsOrEq',\n",
    "    'Associate_degree': 'associateDegree',\n",
    "    'Bachelor_degree': 'bachelorDegree',\n",
    "    'Master_degree': 'masterDegree',\n",
    "    'Doctorate_degree': 'doctorateDegree',\n",
    "    'No_requirement': 'noRequirement',\n",
    "    'Professional_degree': 'professionalDegree',\n",
    "}\n",
    "\n",
    "# Thread-safe logging\n",
    "print_lock = threading.Lock()\n",
    "def log(msg):\n",
    "    with print_lock:\n",
    "        print(msg)\n",
    "\n",
    "def current_timestamp():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def read_and_reshape(input_file):\n",
    "    \"\"\"Reads the CSV and pivots to {occCode: {fields...}} for DynamoDB\"\"\"\n",
    "    edu_data = defaultdict(dict)\n",
    "    for row in csv.DictReader(open(input_file, newline='', encoding='utf-8')):\n",
    "        occ = row['SOC']\n",
    "        code = row['ESTIMATECODE']\n",
    "        field = CODE_TO_FIELD.get(code)\n",
    "        if not field:\n",
    "            continue\n",
    "        value = row['ESTIMATE']\n",
    "        edu_data[occ].setdefault('occCode', occ)\n",
    "        edu_data[occ][field] = value\n",
    "\n",
    "    # Add timestamps to each record\n",
    "    now = current_timestamp()\n",
    "    for record in edu_data.values():\n",
    "        record['createdAt'] = now\n",
    "        record['updatedAt'] = now\n",
    "\n",
    "    return list(edu_data.values())\n",
    "\n",
    "def dynamodb_batch_write(table, items):\n",
    "    with table.batch_writer(overwrite_by_pkeys=['occCode']) as batch:\n",
    "        for item in items:\n",
    "            batch.put_item(Item=item)\n",
    "\n",
    "def batch_iterable(iterable, batch_size):\n",
    "    batch = []\n",
    "    for item in iterable:\n",
    "        batch.append(item)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "def insert_batches_singlethreaded(items, table):\n",
    "    start = time.time()\n",
    "    total = len(items)\n",
    "    batch_num = 0\n",
    "    for batch in batch_iterable(items, 25):\n",
    "        batch_num += 1\n",
    "        try:\n",
    "            dynamodb_batch_write(table, batch)\n",
    "            log(f\"✅ Batch {batch_num} ({len(batch)}) inserted [{(batch_num-1)*25+1}-{batch_num*25}]\")\n",
    "        except ClientError as e:\n",
    "            log(f\"❌ Batch {batch_num} error: {e}\")\n",
    "    log(f\"Total items inserted: {total}\")\n",
    "    return time.time() - start\n",
    "\n",
    "def insert_batches_multithreaded(items, table, n_workers=4):\n",
    "    start = time.time()\n",
    "    batches = list(batch_iterable(items, 100))\n",
    "    batch_num = 0\n",
    "    def upload_batch(batch):\n",
    "        nonlocal batch_num\n",
    "        batch_num += 1\n",
    "        try:\n",
    "            dynamodb_batch_write(table, batch)\n",
    "            log(f\"✅ Batch {batch_num} ({len(batch)}) inserted\")\n",
    "        except ClientError as e:\n",
    "            log(f\"❌ Batch {batch_num} error: {e}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = [executor.submit(upload_batch, batch) for batch in batches]\n",
    "        for f in as_completed(futures):\n",
    "            pass\n",
    "    log(f\"Total items inserted: {len(items)}\")\n",
    "    return time.time() - start\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Read and reshape input\n",
    "    log(f\"🔄 Reading and pivoting CSV: {input_file}\")\n",
    "    items = read_and_reshape(input_file)\n",
    "    log(f\"📦 Total DynamoDB items to insert: {len(items)}\")\n",
    "\n",
    "    # 2. Setup DynamoDB\n",
    "    session = boto3.Session(region_name=region)\n",
    "    dynamodb = session.resource('dynamodb')\n",
    "    table = dynamodb.Table(CAREER_EDUCATION_TABLE)\n",
    "\n",
    "    # 3. Single-threaded insert\n",
    "    log(\"🚀 Starting single-threaded DynamoDB insert...\")\n",
    "    t1 = insert_batches_singlethreaded(items, table)\n",
    "    log(f\"⏱️ Single-threaded insert time: {t1:.2f} sec\")\n",
    "\n",
    "    # 4. Multi-threaded insert\n",
    "    log(\"🚀 Starting multi-threaded DynamoDB insert (4 threads)...\")\n",
    "    t2 = insert_batches_multithreaded(items, table, n_workers=4)\n",
    "    log(f\"⏱️ Multi-threaded insert time: {t2:.2f} sec\")\n",
    "\n",
    "    log(\"🎯 Script completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c36f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting upload from: Datasets/skills_data.csv\n",
      "✅ Batch 1 inserted in 1.53s (100 records)\n",
      "✅ Batch 1 inserted in 1.52s (100 records)\n",
      "✅ Batch 1 inserted in 1.73s (100 records)\n",
      "✅ Batch 1 inserted in 1.74s (100 records)\n",
      "✅ Batch 2 inserted in 0.36s (100 records)\n",
      "✅ Batch 2 inserted in 0.31s (100 records)\n",
      "✅ Batch 2 inserted in 0.42s (100 records)\n",
      "✅ Batch 2 inserted in 0.26s (63 records)\n",
      "🎉 All batches uploaded in 2.13 seconds.\n"
     ]
    }
   ],
   "source": [
    "#insert career skills data\n",
    "import os\n",
    "import csv\n",
    "import ast\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timezone\n",
    "import boto3\n",
    "from queue import Queue\n",
    "\n",
    "# CONFIGURATION\n",
    "CSV_FILE = \"Datasets/skills_data.csv\"\n",
    "REGION = \"us-west-2\"\n",
    "BATCH_SIZE = 100\n",
    "NUM_THREADS = 4\n",
    "REJECTED_FILE = \"rejected_career_skills.csv\"\n",
    "\n",
    "# AWS setup\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=REGION)\n",
    "table = dynamodb.Table(CAREER_SKILLS_TABLE)\n",
    "\n",
    "queue = Queue()\n",
    "rejected_rows = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def current_timestamp():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def process_batch(batch, batch_number):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        with table.batch_writer(overwrite_by_pkeys=['occCode']) as writer:\n",
    "            for item in batch:\n",
    "                try:\n",
    "                    writer.put_item(Item=item)\n",
    "                except Exception as e:\n",
    "                    with lock:\n",
    "                        rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"✅ Batch {batch_number} inserted in {time.time() - start:.2f}s ({len(batch)} records)\")\n",
    "    except Exception as e:\n",
    "        with lock:\n",
    "            for item in batch:\n",
    "                rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"❌ Batch {batch_number} failed with error: {str(e)}\")\n",
    "\n",
    "def worker():\n",
    "    batch_number = 1\n",
    "    while True:\n",
    "        batch = queue.get()\n",
    "        if batch is None:\n",
    "            break\n",
    "        process_batch(batch, batch_number)\n",
    "        batch_number += 1\n",
    "        queue.task_done()\n",
    "\n",
    "def read_and_queue_csv(filepath):\n",
    "    with open(filepath, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        batch = []\n",
    "        for row_num, row in enumerate(reader, 2):\n",
    "            try:\n",
    "                occ_code = str(row['SOC_CODE'])\n",
    "                skills_str = row['TYPICAL_SKILLS']\n",
    "                skills = ast.literal_eval(skills_str)\n",
    "                now = current_timestamp()\n",
    "                item = {\n",
    "                    'occCode': occ_code,\n",
    "                    'skills': skills,\n",
    "                    'createdAt': now,\n",
    "                    'updatedAt': now\n",
    "                }\n",
    "                batch.append(item)\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "                    queue.put(batch)\n",
    "                    batch = []\n",
    "                if row_num % 1000 == 0:\n",
    "                    print(f\"   ...Processed {row_num} rows so far\")\n",
    "            except Exception as e:\n",
    "                with lock:\n",
    "                    rejected_rows.append({**row, 'error': str(e)})\n",
    "        if batch:\n",
    "            queue.put(batch)\n",
    "\n",
    "def save_rejected_rows():\n",
    "    if rejected_rows:\n",
    "        keys = rejected_rows[0].keys()\n",
    "        with open(REJECTED_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rejected_rows)\n",
    "        print(f\"⚠️ {len(rejected_rows)} records rejected. Saved to {REJECTED_FILE}\")\n",
    "\n",
    "# Main\n",
    "print(f\"🚀 Starting upload from: {CSV_FILE}\")\n",
    "start = time.time()\n",
    "\n",
    "threads = []\n",
    "for _ in range(NUM_THREADS):\n",
    "    t = threading.Thread(target=worker)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "read_and_queue_csv(CSV_FILE)\n",
    "queue.join()\n",
    "\n",
    "# Stop workers\n",
    "for _ in range(NUM_THREADS):\n",
    "    queue.put(None)\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "save_rejected_rows()\n",
    "print(f\"🎉 All batches uploaded in {time.time() - start:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaba9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting upload from: Datasets/description.csv\n",
      "   ...Processed 1000 rows so far\n",
      "✅ Batch 1 inserted in 1.29s (100 records)\n",
      "✅ Batch 1 inserted in 1.32s (100 records)\n",
      "✅ Batch 1 inserted in 1.33s (100 records)\n",
      "✅ Batch 1 inserted in 1.36s (100 records)\n",
      "✅ Batch 2 inserted in 0.11s (100 records)\n",
      "✅ Batch 2 inserted in 0.13s (100 records)\n",
      "✅ Batch 2 inserted in 0.16s (100 records)\n",
      "✅ Batch 3 inserted in 0.04s (16 records)\n",
      "✅ Batch 2 inserted in 0.18s (100 records)\n",
      "✅ Batch 3 inserted in 0.14s (100 records)\n",
      "✅ Batch 3 inserted in 0.14s (100 records)\n",
      "🎉 All batches uploaded in 1.64 seconds.\n"
     ]
    }
   ],
   "source": [
    "#inserts career description data\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timezone\n",
    "import boto3\n",
    "from queue import Queue\n",
    "\n",
    "# CONFIGURATION\n",
    "CSV_FILE = \"Datasets/description.csv\"\n",
    "REGION = \"us-west-2\"\n",
    "BATCH_SIZE = 100\n",
    "NUM_THREADS = 4\n",
    "REJECTED_FILE = \"rejected_career_description.csv\"\n",
    "\n",
    "# AWS setup\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=REGION)\n",
    "table = dynamodb.Table(CAREER_DESCRIPTION_TABLE)\n",
    "\n",
    "queue = Queue()\n",
    "rejected_rows = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def current_timestamp():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def process_batch(batch, batch_number):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        with table.batch_writer(overwrite_by_pkeys=['occCode']) as writer:\n",
    "            for item in batch:\n",
    "                try:\n",
    "                    writer.put_item(Item=item)\n",
    "                except Exception as e:\n",
    "                    with lock:\n",
    "                        rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"✅ Batch {batch_number} inserted in {time.time() - start:.2f}s ({len(batch)} records)\")\n",
    "    except Exception as e:\n",
    "        with lock:\n",
    "            for item in batch:\n",
    "                rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"❌ Batch {batch_number} failed with error: {str(e)}\")\n",
    "\n",
    "def worker():\n",
    "    batch_number = 1\n",
    "    while True:\n",
    "        batch = queue.get()\n",
    "        if batch is None:\n",
    "            break\n",
    "        process_batch(batch, batch_number)\n",
    "        batch_number += 1\n",
    "        queue.task_done()\n",
    "\n",
    "def read_and_queue_csv(filepath):\n",
    "    with open(filepath, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        batch = []\n",
    "        for row_num, row in enumerate(reader, 2):\n",
    "            try:\n",
    "                now = current_timestamp()\n",
    "                item = {\n",
    "                    'occCode': str(row['Code']),\n",
    "                    'description': str(row['Description']),\n",
    "                    'createdAt': now,\n",
    "                    'updatedAt': now,\n",
    "                }\n",
    "                batch.append(item)\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "                    queue.put(batch)\n",
    "                    batch = []\n",
    "                if row_num % 1000 == 0:\n",
    "                    print(f\"   ...Processed {row_num} rows so far\")\n",
    "            except Exception as e:\n",
    "                with lock:\n",
    "                    rejected_rows.append({**row, 'error': str(e)})\n",
    "        if batch:\n",
    "            queue.put(batch)\n",
    "\n",
    "def save_rejected_rows():\n",
    "    if rejected_rows:\n",
    "        keys = rejected_rows[0].keys()\n",
    "        with open(REJECTED_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rejected_rows)\n",
    "        print(f\"⚠️ {len(rejected_rows)} records rejected. Saved to {REJECTED_FILE}\")\n",
    "\n",
    "# Main\n",
    "print(f\"🚀 Starting upload from: {CSV_FILE}\")\n",
    "start = time.time()\n",
    "\n",
    "threads = []\n",
    "for _ in range(NUM_THREADS):\n",
    "    t = threading.Thread(target=worker)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "read_and_queue_csv(CSV_FILE)\n",
    "queue.join()\n",
    "\n",
    "# Stop workers\n",
    "for _ in range(NUM_THREADS):\n",
    "    queue.put(None)\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "save_rejected_rows()\n",
    "print(f\"🎉 All batches uploaded in {time.time() - start:.2f} seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d17e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting upload from: Datasets/unique_occ_codes.csv\n",
      "   ...Processed 1000 rows so far\n",
      "✅ Batch 1 inserted in 1.06s (100 records)\n",
      "✅ Batch 1 inserted in 1.07s (100 records)\n",
      "✅ Batch 1 inserted in 1.10s (100 records)\n",
      "✅ Batch 1 inserted in 1.09s (100 records)\n",
      "✅ Batch 2 inserted in 0.25s (100 records)\n",
      "✅ Batch 2 inserted in 0.24s (100 records)\n",
      "✅ Batch 2 inserted in 0.27s (100 records)\n",
      "✅ Batch 2 inserted in 0.30s (100 records)\n",
      "✅ Batch 3 inserted in 0.07s (43 records)\n",
      "✅ Batch 3 inserted in 0.18s (100 records)\n",
      "✅ Batch 3 inserted in 0.19s (100 records)\n",
      "🎉 All batches uploaded in 1.56 seconds.\n"
     ]
    }
   ],
   "source": [
    "#insert soc codes data\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timezone\n",
    "import boto3\n",
    "from queue import Queue\n",
    "\n",
    "# CONFIGURATION\n",
    "CSV_FILE = \"Datasets/unique_occ_codes.csv\"\n",
    "REGION = \"us-west-2\"\n",
    "BATCH_SIZE = 100\n",
    "NUM_THREADS = 4\n",
    "REJECTED_FILE = \"rejected_soc_codes.csv\"\n",
    "\n",
    "# AWS setup\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=REGION)\n",
    "table = dynamodb.Table(SOC_CODES_TABLE)\n",
    "\n",
    "queue = Queue()\n",
    "rejected_rows = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def current_timestamp():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def process_batch(batch, batch_number):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        with table.batch_writer(overwrite_by_pkeys=['occCode']) as writer:\n",
    "            for item in batch:\n",
    "                try:\n",
    "                    writer.put_item(Item=item)\n",
    "                except Exception as e:\n",
    "                    with lock:\n",
    "                        rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"✅ Batch {batch_number} inserted in {time.time() - start:.2f}s ({len(batch)} records)\")\n",
    "    except Exception as e:\n",
    "        with lock:\n",
    "            for item in batch:\n",
    "                rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"❌ Batch {batch_number} failed with error: {str(e)}\")\n",
    "\n",
    "def worker():\n",
    "    batch_number = 1\n",
    "    while True:\n",
    "        batch = queue.get()\n",
    "        if batch is None:\n",
    "            break\n",
    "        process_batch(batch, batch_number)\n",
    "        batch_number += 1\n",
    "        queue.task_done()\n",
    "\n",
    "def read_and_queue_csv(filepath):\n",
    "    with open(filepath, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        batch = []\n",
    "        for row_num, row in enumerate(reader, 2):\n",
    "            try:\n",
    "                now = current_timestamp()\n",
    "                item = {\n",
    "                    'occCode': str(row['OCC_CODE']),\n",
    "                    'occTitle': str(row['OCC_TITLE']),\n",
    "                    'createdAt': now,\n",
    "                    'updatedAt': now,\n",
    "                }\n",
    "                batch.append(item)\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "                    queue.put(batch)\n",
    "                    batch = []\n",
    "                if row_num % 1000 == 0:\n",
    "                    print(f\"   ...Processed {row_num} rows so far\")\n",
    "            except Exception as e:\n",
    "                with lock:\n",
    "                    rejected_rows.append({**row, 'error': str(e)})\n",
    "        if batch:\n",
    "            queue.put(batch)\n",
    "\n",
    "def save_rejected_rows():\n",
    "    if rejected_rows:\n",
    "        keys = rejected_rows[0].keys()\n",
    "        with open(REJECTED_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rejected_rows)\n",
    "        print(f\"⚠️ {len(rejected_rows)} records rejected. Saved to {REJECTED_FILE}\")\n",
    "\n",
    "# Main\n",
    "print(f\"🚀 Starting upload from: {CSV_FILE}\")\n",
    "start = time.time()\n",
    "\n",
    "threads = []\n",
    "for _ in range(NUM_THREADS):\n",
    "    t = threading.Thread(target=worker)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "read_and_queue_csv(CSV_FILE)\n",
    "queue.join()\n",
    "\n",
    "# Stop workers\n",
    "for _ in range(NUM_THREADS):\n",
    "    queue.put(None)\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "save_rejected_rows()\n",
    "print(f\"🎉 All batches uploaded in {time.time() - start:.2f} seconds.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
