{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9bbf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this script 1 time per CSV dataset downloaded from here: https://www.bls.gov/oes/tables.htm only download AllData XLSX files for ech year.\n",
    "# once run, you can discard the shitty XLSX files and use the CSV files instead, discard those pieces of shit for good!\n",
    "import os\n",
    "import csv\n",
    "from openpyxl import load_workbook\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "input_dir = \"Datasets/salary\"\n",
    "output_dir = \"Datasets/csv_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print_lock = threading.Lock()\n",
    "\n",
    "def log(msg):\n",
    "    with print_lock:\n",
    "        print(msg)\n",
    "\n",
    "def convert_file(filename):\n",
    "    if not filename.endswith(\".xlsx\"):\n",
    "        return\n",
    "    input_path = os.path.join(input_dir, filename)\n",
    "    output_filename = os.path.splitext(filename)[0] + \".csv\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    log(f\"üîÑ START: {filename}\")\n",
    "    try:\n",
    "        wb = load_workbook(filename=input_path, read_only=True)\n",
    "        ws = wb.active\n",
    "        with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for row in ws.iter_rows(values_only=True):\n",
    "                writer.writerow(row)\n",
    "        log(f\"‚úÖ DONE:  {filename} ‚Üí {output_filename}\")\n",
    "    except Exception as e:\n",
    "        log(f\"‚ùå ERROR: {filename}: {e}\")\n",
    "\n",
    "files = [f for f in os.listdir(input_dir) if f.endswith(\".xlsx\")]\n",
    "\n",
    "log(f\"\\nüöÄ Processing {len(files)} files from '{input_dir}' to '{output_dir}'...\\n\")\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(convert_file, files)\n",
    "log(\"\\nüéØ All XLSX files processed.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09529a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "input_dir = \"Datasets/csv_output\"\n",
    "output_dir = \"Datasets/dynamodb_ready_by_year\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "fieldnames = [\n",
    "    \"occ_code\",\n",
    "    \"salary_key\",\n",
    "    \"a_median\",\n",
    "    \"m_median\",\n",
    "    \"m_pct10\",\n",
    "    \"m_pct90\"\n",
    "]\n",
    "\n",
    "def safe_div(val):\n",
    "    try:\n",
    "        f = float(val)\n",
    "        return f\"{f / 12:.2f}\"\n",
    "    except (ValueError, TypeError):\n",
    "        return \"\"\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if not filename.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    # Extract year from filename, else \"unknown\"\n",
    "    basename = os.path.splitext(filename)[0]\n",
    "    year = None\n",
    "    for part in basename.split('_'):\n",
    "        if part.isdigit() and len(part) == 4:\n",
    "            year = part\n",
    "            break\n",
    "    if not year:\n",
    "        year = \"unknown\"\n",
    "\n",
    "    input_path = os.path.join(input_dir, filename)\n",
    "    output_path = os.path.join(output_dir, f\"{basename}_dynamodb_ready.csv\")\n",
    "\n",
    "    print(f\"üîÑ Converting {filename} to DynamoDB format as {output_path}...\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile, \\\n",
    "         open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "\n",
    "        reader = csv.DictReader(infile)\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        count = 0\n",
    "        log_interval = 10000\n",
    "\n",
    "        for row_num, row in enumerate(reader, 2):\n",
    "            try:\n",
    "                occ_code = row.get('occ_code') or row.get('OCC_CODE')\n",
    "                a_median = row.get('a_median') or row.get('A_MEDIAN')\n",
    "                a_pct10 = row.get('a_pct10') or row.get('A_PCT10')\n",
    "                a_pct90 = row.get('a_pct90') or row.get('A_PCT90')\n",
    "                # Key fields\n",
    "                area_title = row.get('area_title') or row.get('AREA_TITLE')\n",
    "                naics_title = row.get('naics_title') or row.get('NAICS_TITLE')\n",
    "                salary_key = f\"{year}#{area_title}#{naics_title}\"\n",
    "\n",
    "                m_median = safe_div(a_median)\n",
    "                m_pct10 = safe_div(a_pct10)\n",
    "                m_pct90 = safe_div(a_pct90)\n",
    "\n",
    "                writer.writerow({\n",
    "                    \"occ_code\": occ_code,\n",
    "                    \"salary_key\": salary_key,\n",
    "                    \"a_median\": a_median,\n",
    "                    \"m_median\": m_median,\n",
    "                    \"m_pct10\": m_pct10,\n",
    "                    \"m_pct90\": m_pct90,\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "                if count % log_interval == 0:\n",
    "                    print(f\"   ...Processed {count} rows so far in {filename}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed on row {row_num} in {filename}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Wrote {count} rows to {output_path}\")\n",
    "\n",
    "print(\"\\nüéØ All files converted and saved to:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import threading\n",
    "from decimal import Decimal\n",
    "import boto3\n",
    "from queue import Queue\n",
    "\n",
    "# Config\n",
    "CLEANED_DIR = \"Datasets/dynamodb_ready_by_year\"\n",
    "TABLE_NAME = \"careerSalary-alek4h7jlreffeoe5tocxgnx2u-NONE\"\n",
    "REGION = \"us-west-2\"\n",
    "BATCH_SIZE = 1000\n",
    "NUM_THREADS = 4\n",
    "REJECTED_FILE = \"rejected_dynamodb_rows.csv\"\n",
    "\n",
    "# AWS setup\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=REGION)\n",
    "table = dynamodb.Table(TABLE_NAME)\n",
    "\n",
    "queue = Queue()\n",
    "rejected_rows = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def to_decimal(val):\n",
    "    try:\n",
    "        return Decimal(str(round(float(val), 2))) if val not in [None, '', 'null'] else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def process_batch(batch, batch_number):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        with table.batch_writer(overwrite_by_pkeys=['occ_code']) as writer:\n",
    "            for item in batch:\n",
    "                try:\n",
    "                    writer.put_item(Item=item)\n",
    "                except Exception as e:\n",
    "                    with lock:\n",
    "                        rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚úÖ Batch {batch_number} inserted in {time.time() - start:.2f}s ({len(batch)} records)\")\n",
    "    except Exception as e:\n",
    "        with lock:\n",
    "            for item in batch:\n",
    "                rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚ùå Batch {batch_number} failed with error: {str(e)}\")\n",
    "\n",
    "def worker():\n",
    "    batch_number = 1\n",
    "    while True:\n",
    "        batch = queue.get()\n",
    "        if batch is None:\n",
    "            break\n",
    "        process_batch(batch, batch_number)\n",
    "        batch_number += 1\n",
    "        queue.task_done()\n",
    "\n",
    "def read_and_queue_csv(filepath):\n",
    "    with open(filepath, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        batch = []\n",
    "        for row in reader:\n",
    "            try:\n",
    "                item = {\n",
    "                    'occ_code': str(row['occ_code']),\n",
    "                    'salary_key': str(row['salary_key']),\n",
    "                    'a_median': to_decimal(row.get('a_median')),\n",
    "                    'm_median': to_decimal(row.get('m_median')),\n",
    "                    'm_pct10': to_decimal(row.get('m_pct10')),\n",
    "                    'm_pct90': to_decimal(row.get('m_pct90')),\n",
    "                }\n",
    "                # Remove None fields for optional floats (cleaner for DynamoDB)\n",
    "                item = {k: v for k, v in item.items() if v is not None}\n",
    "                batch.append(item)\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "                    queue.put(batch)\n",
    "                    batch = []\n",
    "            except Exception as e:\n",
    "                with lock:\n",
    "                    rejected_rows.append({**row, 'error': str(e)})\n",
    "\n",
    "        if batch:\n",
    "            queue.put(batch)\n",
    "\n",
    "def save_rejected_rows():\n",
    "    if rejected_rows:\n",
    "        keys = rejected_rows[0].keys()\n",
    "        with open(REJECTED_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rejected_rows)\n",
    "        print(f\"‚ö†Ô∏è {len(rejected_rows)} records rejected. Saved to {REJECTED_FILE}\")\n",
    "\n",
    "# Main\n",
    "print(f\"üöÄ Starting upload from all CSVs in: {CLEANED_DIR}\")\n",
    "start = time.time()\n",
    "\n",
    "threads = []\n",
    "for _ in range(NUM_THREADS):\n",
    "    t = threading.Thread(target=worker)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "for filename in os.listdir(CLEANED_DIR):\n",
    "    if filename.endswith(\"_dynamodb_ready.csv\"):\n",
    "        filepath = os.path.join(CLEANED_DIR, filename)\n",
    "        print(f\"üìÇ Queuing data from {filename}\")\n",
    "        read_and_queue_csv(filepath)\n",
    "\n",
    "queue.join()\n",
    "\n",
    "# Stop workers\n",
    "for _ in range(NUM_THREADS):\n",
    "    queue.put(None)\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "save_rejected_rows()\n",
    "print(f\"üéâ All batches uploaded in {time.time() - start:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc8c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert education data in DynamoDB\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "import boto3\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Input CSV and table config\n",
    "input_file = \"Datasets/education_data.csv\"\n",
    "dynamodb_table = \"careerEducation-alek4h7jlreffeoe5tocxgnx2u-NONE\" #replace this line with your table name\n",
    "region = \"us-west-2\"\n",
    "\n",
    "# Map ESTIMATECODE to fieldnames in schema\n",
    "CODE_TO_FIELD = {\n",
    "    'Less_than_hs': 'less_than_highschool',\n",
    "    'hs_or_eq': 'high_school_or_equivalent',\n",
    "    'Associate_degree': 'associate_degree',\n",
    "    'Bachelor_degree': 'bachelor_degree',\n",
    "    'Master_degree': 'master_degree',\n",
    "    'Doctorate_degree': 'doctorate_degree',\n",
    "    'No_requirement': 'no_requirement',\n",
    "    'Professional_degree': 'professional_degree',\n",
    "}\n",
    "\n",
    "# Thread-safe logging\n",
    "print_lock = threading.Lock()\n",
    "def log(msg):\n",
    "    with print_lock:\n",
    "        print(msg)\n",
    "\n",
    "def read_and_reshape(input_file):\n",
    "    \"\"\" Reads the CSV and pivots to {occ_code: {fields...}} for DynamoDB \"\"\"\n",
    "    edu_data = defaultdict(dict)\n",
    "    with open(input_file, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            occ = row['SOC']\n",
    "            code = row['ESTIMATECODE']\n",
    "            field = CODE_TO_FIELD.get(code)\n",
    "            if not field:\n",
    "                continue\n",
    "            value = row['ESTIMATE']\n",
    "            edu_data[occ].setdefault('occ_code', occ)\n",
    "            edu_data[occ][field] = value\n",
    "    return list(edu_data.values())\n",
    "\n",
    "def dynamodb_batch_write(table, items):\n",
    "    with table.batch_writer(overwrite_by_pkeys=['occ_code']) as batch:\n",
    "        for item in items:\n",
    "            batch.put_item(Item=item)\n",
    "\n",
    "def batch_iterable(iterable, batch_size):\n",
    "    batch = []\n",
    "    for item in iterable:\n",
    "        batch.append(item)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "def insert_batches_singlethreaded(items, table):\n",
    "    start = time.time()\n",
    "    total = len(items)\n",
    "    batch_num = 0\n",
    "    for batch in batch_iterable(items, 25):\n",
    "        batch_num += 1\n",
    "        try:\n",
    "            dynamodb_batch_write(table, batch)\n",
    "            log(f\"‚úÖ Batch {batch_num} ({len(batch)}) inserted [{(batch_num-1)*25+1}-{batch_num*25}]\")\n",
    "        except ClientError as e:\n",
    "            log(f\"‚ùå Batch {batch_num} error: {e}\")\n",
    "    log(f\"Total items inserted: {total}\")\n",
    "    return time.time() - start\n",
    "\n",
    "def insert_batches_multithreaded(items, table, n_workers=4):\n",
    "    start = time.time()\n",
    "    batches = list(batch_iterable(items, 100))\n",
    "    batch_num = 0\n",
    "    def upload_batch(batch):\n",
    "        nonlocal batch_num\n",
    "        batch_num += 1\n",
    "        try:\n",
    "            dynamodb_batch_write(table, batch)\n",
    "            log(f\"‚úÖ Batch {batch_num} ({len(batch)}) inserted\")\n",
    "        except ClientError as e:\n",
    "            log(f\"‚ùå Batch {batch_num} error: {e}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = [executor.submit(upload_batch, batch) for batch in batches]\n",
    "        for f in as_completed(futures):\n",
    "            pass\n",
    "    log(f\"Total items inserted: {len(items)}\")\n",
    "    return time.time() - start\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Read and reshape input\n",
    "    log(f\"üîÑ Reading and pivoting CSV: {input_file}\")\n",
    "    items = read_and_reshape(input_file)\n",
    "    log(f\"üì¶ Total DynamoDB items to insert: {len(items)}\")\n",
    "\n",
    "    # 2. Setup DynamoDB\n",
    "    session = boto3.Session(region_name=region)\n",
    "    dynamodb = session.resource('dynamodb')\n",
    "    table = dynamodb.Table(dynamodb_table)\n",
    "\n",
    "    # 3. Single-threaded insert and timing\n",
    "    log(\"üöÄ Starting single-threaded DynamoDB insert...\")\n",
    "    t1 = insert_batches_singlethreaded(items, table)\n",
    "    log(f\"‚è±Ô∏è Single-threaded insert time: {t1:.2f} sec\")\n",
    "\n",
    "    # 4. Multi-threaded insert and timing\n",
    "    log(\"üöÄ Starting multi-threaded DynamoDB insert (4 threads)...\")\n",
    "    t2 = insert_batches_multithreaded(items, table, n_workers=4)\n",
    "    log(f\"‚è±Ô∏è Multi-threaded insert time: {t2:.2f} sec\")\n",
    "\n",
    "    log(\"üéØ Script completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c36f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert skills data in the database\n",
    "import os\n",
    "import csv\n",
    "import ast\n",
    "import time\n",
    "import threading\n",
    "import boto3\n",
    "from decimal import Decimal\n",
    "from queue import Queue\n",
    "\n",
    "# CONFIGURATION\n",
    "CSV_FILE = \"Datasets/skills_data.csv\"   # Path to your skills CSV file\n",
    "TABLE_NAME = \"careerSkills-alek4h7jlreffeoe5tocxgnx2u-NONE\"  # DynamoDB table name\n",
    "REGION = \"us-west-2\"\n",
    "BATCH_SIZE = 100  # DynamoDB max per batch\n",
    "NUM_THREADS = 4  # Adjust as needed\n",
    "REJECTED_FILE = \"rejected_career_skills.csv\"\n",
    "\n",
    "# AWS setup\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=REGION)\n",
    "table = dynamodb.Table(TABLE_NAME)\n",
    "\n",
    "queue = Queue()\n",
    "rejected_rows = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def process_batch(batch, batch_number):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        with table.batch_writer(overwrite_by_pkeys=['occ_code']) as writer:\n",
    "            for item in batch:\n",
    "                try:\n",
    "                    writer.put_item(Item=item)\n",
    "                except Exception as e:\n",
    "                    with lock:\n",
    "                        rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚úÖ Batch {batch_number} inserted in {time.time() - start:.2f}s ({len(batch)} records)\")\n",
    "    except Exception as e:\n",
    "        with lock:\n",
    "            for item in batch:\n",
    "                rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚ùå Batch {batch_number} failed with error: {str(e)}\")\n",
    "\n",
    "def worker():\n",
    "    batch_number = 1\n",
    "    while True:\n",
    "        batch = queue.get()\n",
    "        if batch is None:\n",
    "            break\n",
    "        process_batch(batch, batch_number)\n",
    "        batch_number += 1\n",
    "        queue.task_done()\n",
    "\n",
    "def read_and_queue_csv(filepath):\n",
    "    with open(filepath, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        batch = []\n",
    "        for row_num, row in enumerate(reader, 2):\n",
    "            try:\n",
    "                occ_code = str(row['SOC_CODE'])\n",
    "                skills_str = row['TYPICAL_SKILLS']\n",
    "                # Convert the stringified Python list to an actual list\n",
    "                skills = ast.literal_eval(skills_str)\n",
    "                item = {\n",
    "                    'occ_code': occ_code,\n",
    "                    'skills': skills\n",
    "                }\n",
    "                batch.append(item)\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "                    queue.put(batch)\n",
    "                    batch = []\n",
    "                if row_num % 1000 == 0:\n",
    "                    print(f\"   ...Processed {row_num} rows so far\")\n",
    "            except Exception as e:\n",
    "                with lock:\n",
    "                    rejected_rows.append({**row, 'error': str(e)})\n",
    "        if batch:\n",
    "            queue.put(batch)\n",
    "\n",
    "def save_rejected_rows():\n",
    "    if rejected_rows:\n",
    "        keys = rejected_rows[0].keys()\n",
    "        with open(REJECTED_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rejected_rows)\n",
    "        print(f\"‚ö†Ô∏è {len(rejected_rows)} records rejected. Saved to {REJECTED_FILE}\")\n",
    "\n",
    "# Main\n",
    "print(f\"üöÄ Starting upload from: {CSV_FILE}\")\n",
    "start = time.time()\n",
    "\n",
    "threads = []\n",
    "for _ in range(NUM_THREADS):\n",
    "    t = threading.Thread(target=worker)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "read_and_queue_csv(CSV_FILE)\n",
    "queue.join()\n",
    "\n",
    "# Stop workers\n",
    "for _ in range(NUM_THREADS):\n",
    "    queue.put(None)\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "save_rejected_rows()\n",
    "print(f\"üéâ All batches uploaded in {time.time() - start:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaba9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserts career description data\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import threading\n",
    "import boto3\n",
    "from queue import Queue\n",
    "\n",
    "# CONFIGURATION\n",
    "CSV_FILE = \"Datasets/description.csv\"   # Update this to your CSV file\n",
    "TABLE_NAME = \"careerDescription-alek4h7jlreffeoe5tocxgnx2u-NONE\"  # Update to your DynamoDB table name\n",
    "REGION = \"us-west-2\"\n",
    "BATCH_SIZE = 100  # DynamoDB max per batch write\n",
    "NUM_THREADS = 4\n",
    "REJECTED_FILE = \"rejected_career_description.csv\"\n",
    "\n",
    "# AWS setup\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=REGION)\n",
    "table = dynamodb.Table(TABLE_NAME)\n",
    "\n",
    "queue = Queue()\n",
    "rejected_rows = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def process_batch(batch, batch_number):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        with table.batch_writer(overwrite_by_pkeys=['occ_code']) as writer:\n",
    "            for item in batch:\n",
    "                try:\n",
    "                    writer.put_item(Item=item)\n",
    "                except Exception as e:\n",
    "                    with lock:\n",
    "                        rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚úÖ Batch {batch_number} inserted in {time.time() - start:.2f}s ({len(batch)} records)\")\n",
    "    except Exception as e:\n",
    "        with lock:\n",
    "            for item in batch:\n",
    "                rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚ùå Batch {batch_number} failed with error: {str(e)}\")\n",
    "\n",
    "def worker():\n",
    "    batch_number = 1\n",
    "    while True:\n",
    "        batch = queue.get()\n",
    "        if batch is None:\n",
    "            break\n",
    "        process_batch(batch, batch_number)\n",
    "        batch_number += 1\n",
    "        queue.task_done()\n",
    "\n",
    "def read_and_queue_csv(filepath):\n",
    "    with open(filepath, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        batch = []\n",
    "        for row_num, row in enumerate(reader, 2):\n",
    "            try:\n",
    "                item = {\n",
    "                    'occ_code': str(row['Code']),\n",
    "                    'description': str(row['Description']),\n",
    "                }\n",
    "                batch.append(item)\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "                    queue.put(batch)\n",
    "                    batch = []\n",
    "                if row_num % 1000 == 0:\n",
    "                    print(f\"   ...Processed {row_num} rows so far\")\n",
    "            except Exception as e:\n",
    "                with lock:\n",
    "                    rejected_rows.append({**row, 'error': str(e)})\n",
    "        if batch:\n",
    "            queue.put(batch)\n",
    "\n",
    "def save_rejected_rows():\n",
    "    if rejected_rows:\n",
    "        keys = rejected_rows[0].keys()\n",
    "        with open(REJECTED_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rejected_rows)\n",
    "        print(f\"‚ö†Ô∏è {len(rejected_rows)} records rejected. Saved to {REJECTED_FILE}\")\n",
    "\n",
    "# Main\n",
    "print(f\"üöÄ Starting upload from: {CSV_FILE}\")\n",
    "start = time.time()\n",
    "\n",
    "threads = []\n",
    "for _ in range(NUM_THREADS):\n",
    "    t = threading.Thread(target=worker)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "read_and_queue_csv(CSV_FILE)\n",
    "queue.join()\n",
    "\n",
    "# Stop workers\n",
    "for _ in range(NUM_THREADS):\n",
    "    queue.put(None)\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "save_rejected_rows()\n",
    "print(f\"üéâ All batches uploaded in {time.time() - start:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d17e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserts soc codes data in the database\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import threading\n",
    "import boto3\n",
    "from queue import Queue\n",
    "\n",
    "# CONFIGURATION\n",
    "CSV_FILE = \"Datasets/unique_occ_codes.csv\"   # Update this to your CSV file\n",
    "TABLE_NAME = \"socCodes-alek4h7jlreffeoe5tocxgnx2u-NONE\"  # Update to your DynamoDB table name\n",
    "REGION = \"us-west-2\"\n",
    "BATCH_SIZE = 100\n",
    "NUM_THREADS = 4\n",
    "REJECTED_FILE = \"rejected_soc_codes.csv\"\n",
    "\n",
    "# AWS setup\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=REGION)\n",
    "table = dynamodb.Table(TABLE_NAME)\n",
    "\n",
    "queue = Queue()\n",
    "rejected_rows = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def process_batch(batch, batch_number):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        with table.batch_writer(overwrite_by_pkeys=['occ_code']) as writer:\n",
    "            for item in batch:\n",
    "                try:\n",
    "                    writer.put_item(Item=item)\n",
    "                except Exception as e:\n",
    "                    with lock:\n",
    "                        rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚úÖ Batch {batch_number} inserted in {time.time() - start:.2f}s ({len(batch)} records)\")\n",
    "    except Exception as e:\n",
    "        with lock:\n",
    "            for item in batch:\n",
    "                rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚ùå Batch {batch_number} failed with error: {str(e)}\")\n",
    "\n",
    "def worker():\n",
    "    batch_number = 1\n",
    "    while True:\n",
    "        batch = queue.get()\n",
    "        if batch is None:\n",
    "            break\n",
    "        process_batch(batch, batch_number)\n",
    "        batch_number += 1\n",
    "        queue.task_done()\n",
    "\n",
    "def read_and_queue_csv(filepath):\n",
    "    with open(filepath, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        batch = []\n",
    "        for row_num, row in enumerate(reader, 2):\n",
    "            try:\n",
    "                item = {\n",
    "                    'occ_code': str(row['OCC_CODE']),\n",
    "                    'occ_title': str(row['OCC_TITLE']),\n",
    "                }\n",
    "                batch.append(item)\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "                    queue.put(batch)\n",
    "                    batch = []\n",
    "                if row_num % 1000 == 0:\n",
    "                    print(f\"   ...Processed {row_num} rows so far\")\n",
    "            except Exception as e:\n",
    "                with lock:\n",
    "                    rejected_rows.append({**row, 'error': str(e)})\n",
    "        if batch:\n",
    "            queue.put(batch)\n",
    "\n",
    "def save_rejected_rows():\n",
    "    if rejected_rows:\n",
    "        keys = rejected_rows[0].keys()\n",
    "        with open(REJECTED_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rejected_rows)\n",
    "        print(f\"‚ö†Ô∏è {len(rejected_rows)} records rejected. Saved to {REJECTED_FILE}\")\n",
    "\n",
    "# Main\n",
    "print(f\"üöÄ Starting upload from: {CSV_FILE}\")\n",
    "start = time.time()\n",
    "\n",
    "threads = []\n",
    "for _ in range(NUM_THREADS):\n",
    "    t = threading.Thread(target=worker)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "read_and_queue_csv(CSV_FILE)\n",
    "queue.join()\n",
    "\n",
    "# Stop workers\n",
    "for _ in range(NUM_THREADS):\n",
    "    queue.put(None)\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "save_rejected_rows()\n",
    "print(f\"üéâ All batches uploaded in {time.time() - start:.2f} seconds.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
