{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09529a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ID = \"alek4h7jlreffeoe5tocxgnx2u\"\n",
    "CAREER_SALARY_TABLE      = f\"careerSalary-{API_ID}-NONE\"\n",
    "CAREER_EDUCATION_TABLE   = f\"careerEducation-{API_ID}-NONE\"\n",
    "CAREER_SKILLS_TABLE      = f\"careerSkills-{API_ID}-NONE\"\n",
    "CAREER_DESCRIPTION_TABLE = f\"careerDescription-{API_ID}-NONE\"\n",
    "SOC_CODES_TABLE          = f\"socCodes-{API_ID}-NONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting upload from all CSVs in: Datasets/dynamodb_ready_by_year\n",
      "üìÇ Queuing data from oes_research_2016_allsectors_dynamodb_ready.csv\n",
      "‚úÖ Batch 1 inserted in 4.06s (1000 records)\n",
      "‚úÖ Batch 1 inserted in 4.30s (1000 records)\n",
      "‚úÖ Batch 1 inserted in 4.40s (1000 records)\n",
      "‚úÖ Batch 1 inserted in 4.48s (1000 records)\n",
      "‚úÖ Batch 2 inserted in 3.35s (1000 records)\n",
      "‚úÖ Batch 2 inserted in 3.43s (1000 records)\n",
      "‚úÖ Batch 2 inserted in 3.65s (1000 records)\n",
      "‚úÖ Batch 2 inserted in 3.99s (1000 records)\n",
      "‚úÖ Batch 3 inserted in 3.31s (1000 records)\n",
      "‚úÖ Batch 3 inserted in 3.66s (1000 records)\n",
      "‚úÖ Batch 3 inserted in 3.19s (1000 records)\n",
      "‚úÖ Batch 3 inserted in 3.83s (1000 records)\n",
      "üìÇ Queuing data from oes_research_2017_allsectors_dynamodb_ready.csv\n",
      "‚úÖ Batch 4 inserted in 2.68s (1000 records)\n",
      "‚úÖ Batch 4 inserted in 3.09s (1000 records)\n",
      "‚úÖ Batch 4 inserted in 2.91s (1000 records)\n",
      "‚úÖ Batch 4 inserted in 3.66s (1000 records)\n",
      "‚úÖ Batch 5 inserted in 3.29s (1000 records)\n",
      "‚úÖ Batch 5 inserted in 3.81s (1000 records)\n",
      "‚úÖ Batch 5 inserted in 4.76s (1000 records)\n",
      "‚úÖ Batch 5 inserted in 4.19s (1000 records)\n",
      "‚úÖ Batch 6 inserted in 3.37s (1000 records)\n",
      "‚úÖ Batch 6 inserted in 2.98s (1000 records)\n",
      "‚úÖ Batch 6 inserted in 2.61s (1000 records)\n",
      "‚úÖ Batch 6 inserted in 2.48s (1000 records)\n",
      "‚úÖ Batch 7 inserted in 2.44s (1000 records)\n",
      "‚úÖ Batch 7 inserted in 2.30s (1000 records)\n",
      "‚úÖ Batch 7 inserted in 2.11s (1000 records)\n",
      "‚úÖ Batch 7 inserted in 2.10s (1000 records)\n",
      "‚úÖ Batch 8 inserted in 1.89s (1000 records)\n",
      "‚úÖ Batch 8 inserted in 2.50s (1000 records)\n",
      "‚úÖ Batch 8 inserted in 2.42s (1000 records)\n",
      "‚úÖ Batch 8 inserted in 2.50s (1000 records)\n",
      "‚úÖ Batch 9 inserted in 3.14s (1000 records)\n",
      "‚úÖ Batch 9 inserted in 2.99s (1000 records)\n",
      "‚úÖ Batch 9 inserted in 2.69s (1000 records)\n",
      "‚úÖ Batch 9 inserted in 2.81s (1000 records)\n",
      "‚úÖ Batch 10 inserted in 1.94s (1000 records)\n",
      "‚úÖ Batch 10 inserted in 2.99s (1000 records)\n",
      "‚úÖ Batch 10 inserted in 2.85s (1000 records)\n",
      "‚úÖ Batch 10 inserted in 3.00s (1000 records)\n",
      "‚úÖ Batch 11 inserted in 2.91s (1000 records)\n",
      "üìÇ Queuing data from oes_research_2018_allsectors_dynamodb_ready.csv\n",
      "‚úÖ Batch 11 inserted in 3.51s (1000 records)\n",
      "‚úÖ Batch 11 inserted in 3.64s (1000 records)\n",
      "‚úÖ Batch 11 inserted in 3.56s (1000 records)\n",
      "‚úÖ Batch 12 inserted in 3.58s (1000 records)\n",
      "‚úÖ Batch 13 inserted in 2.43s (1000 records)\n",
      "‚úÖ Batch 12 inserted in 3.02s (1000 records)\n",
      "‚úÖ Batch 12 inserted in 3.08s (1000 records)\n",
      "‚úÖ Batch 12 inserted in 3.20s (1000 records)\n",
      "‚úÖ Batch 14 inserted in 2.72s (1000 records)\n",
      "‚úÖ Batch 13 inserted in 2.70s (1000 records)‚úÖ Batch 13 inserted in 2.55s (1000 records)\n",
      "\n",
      "‚úÖ Batch 13 inserted in 2.61s (1000 records)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    112\u001b[39m         filepath = os.path.join(CLEANED_DIR, filename)\n\u001b[32m    113\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìÇ Queuing data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[43mread_and_queue_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m queue.join()\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Stop workers\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mread_and_queue_csv\u001b[39m\u001b[34m(filepath)\u001b[39m\n\u001b[32m     63\u001b[39m reader = csv.DictReader(f)\n\u001b[32m     64\u001b[39m batch = []\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnow\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_timestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\trupti\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\csv.py:178\u001b[39m, in \u001b[36mDictReader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.line_num == \u001b[32m0\u001b[39m:\n\u001b[32m    176\u001b[39m     \u001b[38;5;66;03m# Used only for its side effect.\u001b[39;00m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28mself\u001b[39m.fieldnames\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m row = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28mself\u001b[39m.line_num = \u001b[38;5;28mself\u001b[39m.reader.line_num\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# unlike the basic reader, we prefer not to return blanks,\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# because we will typically wind up with a dict full of None\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# values\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:322\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 15 inserted in 1.62s (1000 records)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 14 inserted in 1.67s (1000 records)\n",
      "‚úÖ Batch 14 inserted in 1.68s (1000 records)\n",
      "‚úÖ Batch 14 inserted in 1.55s (1000 records)\n",
      "‚úÖ Batch 16 inserted in 1.75s (1000 records)\n",
      "‚úÖ Batch 15 inserted in 1.73s (1000 records)\n",
      "‚úÖ Batch 15 inserted in 1.88s (1000 records)\n",
      "‚úÖ Batch 15 inserted in 1.77s (1000 records)\n",
      "‚úÖ Batch 17 inserted in 1.68s (1000 records)\n",
      "‚úÖ Batch 16 inserted in 1.65s (1000 records)\n",
      "‚úÖ Batch 16 inserted in 1.72s (1000 records)\n",
      "‚úÖ Batch 16 inserted in 1.76s (1000 records)\n",
      "‚úÖ Batch 18 inserted in 1.55s (1000 records)\n",
      "‚úÖ Batch 17 inserted in 1.62s (1000 records)\n",
      "‚úÖ Batch 17 inserted in 1.54s (1000 records)\n",
      "‚úÖ Batch 17 inserted in 1.64s (1000 records)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import threading\n",
    "from decimal import Decimal\n",
    "from datetime import datetime, timezone\n",
    "import boto3\n",
    "from queue import Queue\n",
    "\n",
    "# Config\n",
    "CLEANED_DIR = \"Datasets/dynamodb_ready_by_year\"\n",
    "REGION = \"us-west-2\"\n",
    "BATCH_SIZE = 1000\n",
    "NUM_THREADS = 4\n",
    "REJECTED_FILE = \"rejected_dynamodb_rows.csv\"\n",
    "\n",
    "# AWS setup\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=REGION)\n",
    "table = dynamodb.Table(CAREER_SALARY_TABLE)\n",
    "\n",
    "queue = Queue()\n",
    "rejected_rows = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def to_decimal(val):\n",
    "    try:\n",
    "        return Decimal(str(round(float(val), 2))) if val not in [None, '', 'null'] else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def current_timestamp():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def process_batch(batch, batch_number):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        with table.batch_writer(overwrite_by_pkeys=['occCode']) as writer:\n",
    "            for item in batch:\n",
    "                try:\n",
    "                    writer.put_item(Item=item)\n",
    "                except Exception as e:\n",
    "                    with lock:\n",
    "                        rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚úÖ Batch {batch_number} inserted in {time.time() - start:.2f}s ({len(batch)} records)\")\n",
    "    except Exception as e:\n",
    "        with lock:\n",
    "            for item in batch:\n",
    "                rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚ùå Batch {batch_number} failed with error: {str(e)}\")\n",
    "\n",
    "def worker():\n",
    "    batch_number = 1\n",
    "    while True:\n",
    "        batch = queue.get()\n",
    "        if batch is None:\n",
    "            break\n",
    "        process_batch(batch, batch_number)\n",
    "        batch_number += 1\n",
    "        queue.task_done()\n",
    "\n",
    "def read_and_queue_csv(filepath):\n",
    "    with open(filepath, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        batch = []\n",
    "        for row in reader:\n",
    "            try:\n",
    "                now = current_timestamp()\n",
    "                item = {\n",
    "                    'occCode': str(row['occ_code']),\n",
    "                    'salaryKey': str(row['salary_key']),\n",
    "                    'aMedian': to_decimal(row.get('a_median')),\n",
    "                    'mMedian': to_decimal(row.get('m_median')),\n",
    "                    'mPct10': to_decimal(row.get('m_pct10')),\n",
    "                    'mPct90': to_decimal(row.get('m_pct90')),\n",
    "                    'createdAt': now,\n",
    "                    'updatedAt': now,\n",
    "                }\n",
    "                # Remove None fields\n",
    "                item = {k: v for k, v in item.items() if v is not None}\n",
    "                batch.append(item)\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "                    queue.put(batch)\n",
    "                    batch = []\n",
    "            except Exception as e:\n",
    "                with lock:\n",
    "                    rejected_rows.append({**row, 'error': str(e)})\n",
    "\n",
    "        if batch:\n",
    "            queue.put(batch)\n",
    "\n",
    "def save_rejected_rows():\n",
    "    if rejected_rows:\n",
    "        keys = rejected_rows[0].keys()\n",
    "        with open(REJECTED_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rejected_rows)\n",
    "        print(f\"‚ö†Ô∏è {len(rejected_rows)} records rejected. Saved to {REJECTED_FILE}\")\n",
    "\n",
    "# Main\n",
    "print(f\"üöÄ Starting upload from all CSVs in: {CLEANED_DIR}\")\n",
    "start = time.time()\n",
    "\n",
    "threads = []\n",
    "for _ in range(NUM_THREADS):\n",
    "    t = threading.Thread(target=worker)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "for filename in os.listdir(CLEANED_DIR):\n",
    "    if filename.endswith(\"_dynamodb_ready.csv\"):\n",
    "        filepath = os.path.join(CLEANED_DIR, filename)\n",
    "        print(f\"üìÇ Queuing data from {filename}\")\n",
    "        read_and_queue_csv(filepath)\n",
    "\n",
    "queue.join()\n",
    "\n",
    "# Stop workers\n",
    "for _ in range(NUM_THREADS):\n",
    "    queue.put(None)\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "save_rejected_rows()\n",
    "print(f\"üéâ All batches uploaded in {time.time() - start:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc8c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Reading and pivoting CSV: Datasets/education_data.csv\n",
      "üì¶ Total DynamoDB items to insert: 175\n",
      "üöÄ Starting single-threaded DynamoDB insert...\n",
      "‚úÖ Batch 1 (25) inserted [1-25]\n",
      "‚úÖ Batch 2 (25) inserted [26-50]\n",
      "‚úÖ Batch 3 (25) inserted [51-75]\n",
      "‚úÖ Batch 4 (25) inserted [76-100]\n",
      "‚úÖ Batch 5 (25) inserted [101-125]\n",
      "‚úÖ Batch 6 (25) inserted [126-150]\n",
      "‚úÖ Batch 7 (25) inserted [151-175]\n",
      "Total items inserted: 175\n",
      "‚è±Ô∏è Single-threaded insert time: 1.05 sec\n",
      "üöÄ Starting multi-threaded DynamoDB insert (4 threads)...\n",
      "‚úÖ Batch 2 (100) inserted\n",
      "‚úÖ Batch 2 (75) inserted\n",
      "Total items inserted: 175\n",
      "‚è±Ô∏è Multi-threaded insert time: 0.92 sec\n",
      "üéØ Script completed.\n"
     ]
    }
   ],
   "source": [
    "#insert education data\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timezone\n",
    "import boto3\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Input CSV and table config\n",
    "input_file = \"Datasets/education_data.csv\"\n",
    "region = \"us-west-2\"\n",
    "\n",
    "# Updated field mapping to match new schema (camelCase)\n",
    "CODE_TO_FIELD = {\n",
    "    'Less_than_hs': 'lessThanHs',\n",
    "    'hs_or_eq': 'hsOrEq',\n",
    "    'Associate_degree': 'associateDegree',\n",
    "    'Bachelor_degree': 'bachelorDegree',\n",
    "    'Master_degree': 'masterDegree',\n",
    "    'Doctorate_degree': 'doctorateDegree',\n",
    "    'No_requirement': 'noRequirement',\n",
    "    'Professional_degree': 'professionalDegree',\n",
    "}\n",
    "\n",
    "# Thread-safe logging\n",
    "print_lock = threading.Lock()\n",
    "def log(msg):\n",
    "    with print_lock:\n",
    "        print(msg)\n",
    "\n",
    "def current_timestamp():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def read_and_reshape(input_file):\n",
    "    \"\"\"Reads the CSV and pivots to {occCode: {fields...}} for DynamoDB\"\"\"\n",
    "    edu_data = defaultdict(dict)\n",
    "    for row in csv.DictReader(open(input_file, newline='', encoding='utf-8')):\n",
    "        occ = row['SOC']\n",
    "        code = row['ESTIMATECODE']\n",
    "        field = CODE_TO_FIELD.get(code)\n",
    "        if not field:\n",
    "            continue\n",
    "        value = row['ESTIMATE']\n",
    "        edu_data[occ].setdefault('occCode', occ)\n",
    "        edu_data[occ][field] = value\n",
    "\n",
    "    # Add timestamps to each record\n",
    "    now = current_timestamp()\n",
    "    for record in edu_data.values():\n",
    "        record['createdAt'] = now\n",
    "        record['updatedAt'] = now\n",
    "\n",
    "    return list(edu_data.values())\n",
    "\n",
    "def dynamodb_batch_write(table, items):\n",
    "    with table.batch_writer(overwrite_by_pkeys=['occCode']) as batch:\n",
    "        for item in items:\n",
    "            batch.put_item(Item=item)\n",
    "\n",
    "def batch_iterable(iterable, batch_size):\n",
    "    batch = []\n",
    "    for item in iterable:\n",
    "        batch.append(item)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "def insert_batches_singlethreaded(items, table):\n",
    "    start = time.time()\n",
    "    total = len(items)\n",
    "    batch_num = 0\n",
    "    for batch in batch_iterable(items, 25):\n",
    "        batch_num += 1\n",
    "        try:\n",
    "            dynamodb_batch_write(table, batch)\n",
    "            log(f\"‚úÖ Batch {batch_num} ({len(batch)}) inserted [{(batch_num-1)*25+1}-{batch_num*25}]\")\n",
    "        except ClientError as e:\n",
    "            log(f\"‚ùå Batch {batch_num} error: {e}\")\n",
    "    log(f\"Total items inserted: {total}\")\n",
    "    return time.time() - start\n",
    "\n",
    "def insert_batches_multithreaded(items, table, n_workers=4):\n",
    "    start = time.time()\n",
    "    batches = list(batch_iterable(items, 100))\n",
    "    batch_num = 0\n",
    "    def upload_batch(batch):\n",
    "        nonlocal batch_num\n",
    "        batch_num += 1\n",
    "        try:\n",
    "            dynamodb_batch_write(table, batch)\n",
    "            log(f\"‚úÖ Batch {batch_num} ({len(batch)}) inserted\")\n",
    "        except ClientError as e:\n",
    "            log(f\"‚ùå Batch {batch_num} error: {e}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = [executor.submit(upload_batch, batch) for batch in batches]\n",
    "        for f in as_completed(futures):\n",
    "            pass\n",
    "    log(f\"Total items inserted: {len(items)}\")\n",
    "    return time.time() - start\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Read and reshape input\n",
    "    log(f\"üîÑ Reading and pivoting CSV: {input_file}\")\n",
    "    items = read_and_reshape(input_file)\n",
    "    log(f\"üì¶ Total DynamoDB items to insert: {len(items)}\")\n",
    "\n",
    "    # 2. Setup DynamoDB\n",
    "    session = boto3.Session(region_name=region)\n",
    "    dynamodb = session.resource('dynamodb')\n",
    "    table = dynamodb.Table(CAREER_EDUCATION_TABLE)\n",
    "\n",
    "    # 3. Single-threaded insert\n",
    "    log(\"üöÄ Starting single-threaded DynamoDB insert...\")\n",
    "    t1 = insert_batches_singlethreaded(items, table)\n",
    "    log(f\"‚è±Ô∏è Single-threaded insert time: {t1:.2f} sec\")\n",
    "\n",
    "    # 4. Multi-threaded insert\n",
    "    log(\"üöÄ Starting multi-threaded DynamoDB insert (4 threads)...\")\n",
    "    t2 = insert_batches_multithreaded(items, table, n_workers=4)\n",
    "    log(f\"‚è±Ô∏è Multi-threaded insert time: {t2:.2f} sec\")\n",
    "\n",
    "    log(\"üéØ Script completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c36f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting upload from: Datasets/skills_data.csv\n",
      "‚úÖ Batch 1 inserted in 1.53s (100 records)\n",
      "‚úÖ Batch 1 inserted in 1.52s (100 records)\n",
      "‚úÖ Batch 1 inserted in 1.73s (100 records)\n",
      "‚úÖ Batch 1 inserted in 1.74s (100 records)\n",
      "‚úÖ Batch 2 inserted in 0.36s (100 records)\n",
      "‚úÖ Batch 2 inserted in 0.31s (100 records)\n",
      "‚úÖ Batch 2 inserted in 0.42s (100 records)\n",
      "‚úÖ Batch 2 inserted in 0.26s (63 records)\n",
      "üéâ All batches uploaded in 2.13 seconds.\n"
     ]
    }
   ],
   "source": [
    "#insert career skills data\n",
    "import os\n",
    "import csv\n",
    "import ast\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timezone\n",
    "import boto3\n",
    "from queue import Queue\n",
    "\n",
    "# CONFIGURATION\n",
    "CSV_FILE = \"Datasets/skills_data.csv\"\n",
    "REGION = \"us-west-2\"\n",
    "BATCH_SIZE = 100\n",
    "NUM_THREADS = 4\n",
    "REJECTED_FILE = \"rejected_career_skills.csv\"\n",
    "\n",
    "# AWS setup\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=REGION)\n",
    "table = dynamodb.Table(CAREER_SKILLS_TABLE)\n",
    "\n",
    "queue = Queue()\n",
    "rejected_rows = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def current_timestamp():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def process_batch(batch, batch_number):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        with table.batch_writer(overwrite_by_pkeys=['occCode']) as writer:\n",
    "            for item in batch:\n",
    "                try:\n",
    "                    writer.put_item(Item=item)\n",
    "                except Exception as e:\n",
    "                    with lock:\n",
    "                        rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚úÖ Batch {batch_number} inserted in {time.time() - start:.2f}s ({len(batch)} records)\")\n",
    "    except Exception as e:\n",
    "        with lock:\n",
    "            for item in batch:\n",
    "                rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚ùå Batch {batch_number} failed with error: {str(e)}\")\n",
    "\n",
    "def worker():\n",
    "    batch_number = 1\n",
    "    while True:\n",
    "        batch = queue.get()\n",
    "        if batch is None:\n",
    "            break\n",
    "        process_batch(batch, batch_number)\n",
    "        batch_number += 1\n",
    "        queue.task_done()\n",
    "\n",
    "def read_and_queue_csv(filepath):\n",
    "    with open(filepath, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        batch = []\n",
    "        for row_num, row in enumerate(reader, 2):\n",
    "            try:\n",
    "                occ_code = str(row['SOC_CODE'])\n",
    "                skills_str = row['TYPICAL_SKILLS']\n",
    "                skills = ast.literal_eval(skills_str)\n",
    "                now = current_timestamp()\n",
    "                item = {\n",
    "                    'occCode': occ_code,\n",
    "                    'skills': skills,\n",
    "                    'createdAt': now,\n",
    "                    'updatedAt': now\n",
    "                }\n",
    "                batch.append(item)\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "                    queue.put(batch)\n",
    "                    batch = []\n",
    "                if row_num % 1000 == 0:\n",
    "                    print(f\"   ...Processed {row_num} rows so far\")\n",
    "            except Exception as e:\n",
    "                with lock:\n",
    "                    rejected_rows.append({**row, 'error': str(e)})\n",
    "        if batch:\n",
    "            queue.put(batch)\n",
    "\n",
    "def save_rejected_rows():\n",
    "    if rejected_rows:\n",
    "        keys = rejected_rows[0].keys()\n",
    "        with open(REJECTED_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rejected_rows)\n",
    "        print(f\"‚ö†Ô∏è {len(rejected_rows)} records rejected. Saved to {REJECTED_FILE}\")\n",
    "\n",
    "# Main\n",
    "print(f\"üöÄ Starting upload from: {CSV_FILE}\")\n",
    "start = time.time()\n",
    "\n",
    "threads = []\n",
    "for _ in range(NUM_THREADS):\n",
    "    t = threading.Thread(target=worker)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "read_and_queue_csv(CSV_FILE)\n",
    "queue.join()\n",
    "\n",
    "# Stop workers\n",
    "for _ in range(NUM_THREADS):\n",
    "    queue.put(None)\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "save_rejected_rows()\n",
    "print(f\"üéâ All batches uploaded in {time.time() - start:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaba9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting upload from: Datasets/description.csv\n",
      "   ...Processed 1000 rows so far\n",
      "‚úÖ Batch 1 inserted in 1.29s (100 records)\n",
      "‚úÖ Batch 1 inserted in 1.32s (100 records)\n",
      "‚úÖ Batch 1 inserted in 1.33s (100 records)\n",
      "‚úÖ Batch 1 inserted in 1.36s (100 records)\n",
      "‚úÖ Batch 2 inserted in 0.11s (100 records)\n",
      "‚úÖ Batch 2 inserted in 0.13s (100 records)\n",
      "‚úÖ Batch 2 inserted in 0.16s (100 records)\n",
      "‚úÖ Batch 3 inserted in 0.04s (16 records)\n",
      "‚úÖ Batch 2 inserted in 0.18s (100 records)\n",
      "‚úÖ Batch 3 inserted in 0.14s (100 records)\n",
      "‚úÖ Batch 3 inserted in 0.14s (100 records)\n",
      "üéâ All batches uploaded in 1.64 seconds.\n"
     ]
    }
   ],
   "source": [
    "#inserts career description data\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timezone\n",
    "import boto3\n",
    "from queue import Queue\n",
    "\n",
    "# CONFIGURATION\n",
    "CSV_FILE = \"Datasets/description.csv\"\n",
    "REGION = \"us-west-2\"\n",
    "BATCH_SIZE = 100\n",
    "NUM_THREADS = 4\n",
    "REJECTED_FILE = \"rejected_career_description.csv\"\n",
    "\n",
    "# AWS setup\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=REGION)\n",
    "table = dynamodb.Table(CAREER_DESCRIPTION_TABLE)\n",
    "\n",
    "queue = Queue()\n",
    "rejected_rows = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def current_timestamp():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def process_batch(batch, batch_number):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        with table.batch_writer(overwrite_by_pkeys=['occCode']) as writer:\n",
    "            for item in batch:\n",
    "                try:\n",
    "                    writer.put_item(Item=item)\n",
    "                except Exception as e:\n",
    "                    with lock:\n",
    "                        rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚úÖ Batch {batch_number} inserted in {time.time() - start:.2f}s ({len(batch)} records)\")\n",
    "    except Exception as e:\n",
    "        with lock:\n",
    "            for item in batch:\n",
    "                rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚ùå Batch {batch_number} failed with error: {str(e)}\")\n",
    "\n",
    "def worker():\n",
    "    batch_number = 1\n",
    "    while True:\n",
    "        batch = queue.get()\n",
    "        if batch is None:\n",
    "            break\n",
    "        process_batch(batch, batch_number)\n",
    "        batch_number += 1\n",
    "        queue.task_done()\n",
    "\n",
    "def read_and_queue_csv(filepath):\n",
    "    with open(filepath, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        batch = []\n",
    "        for row_num, row in enumerate(reader, 2):\n",
    "            try:\n",
    "                now = current_timestamp()\n",
    "                item = {\n",
    "                    'occCode': str(row['Code']),\n",
    "                    'description': str(row['Description']),\n",
    "                    'createdAt': now,\n",
    "                    'updatedAt': now,\n",
    "                }\n",
    "                batch.append(item)\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "                    queue.put(batch)\n",
    "                    batch = []\n",
    "                if row_num % 1000 == 0:\n",
    "                    print(f\"   ...Processed {row_num} rows so far\")\n",
    "            except Exception as e:\n",
    "                with lock:\n",
    "                    rejected_rows.append({**row, 'error': str(e)})\n",
    "        if batch:\n",
    "            queue.put(batch)\n",
    "\n",
    "def save_rejected_rows():\n",
    "    if rejected_rows:\n",
    "        keys = rejected_rows[0].keys()\n",
    "        with open(REJECTED_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rejected_rows)\n",
    "        print(f\"‚ö†Ô∏è {len(rejected_rows)} records rejected. Saved to {REJECTED_FILE}\")\n",
    "\n",
    "# Main\n",
    "print(f\"üöÄ Starting upload from: {CSV_FILE}\")\n",
    "start = time.time()\n",
    "\n",
    "threads = []\n",
    "for _ in range(NUM_THREADS):\n",
    "    t = threading.Thread(target=worker)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "read_and_queue_csv(CSV_FILE)\n",
    "queue.join()\n",
    "\n",
    "# Stop workers\n",
    "for _ in range(NUM_THREADS):\n",
    "    queue.put(None)\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "save_rejected_rows()\n",
    "print(f\"üéâ All batches uploaded in {time.time() - start:.2f} seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d17e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting upload from: Datasets/unique_occ_codes.csv\n",
      "   ...Processed 1000 rows so far\n",
      "‚úÖ Batch 1 inserted in 1.06s (100 records)\n",
      "‚úÖ Batch 1 inserted in 1.07s (100 records)\n",
      "‚úÖ Batch 1 inserted in 1.10s (100 records)\n",
      "‚úÖ Batch 1 inserted in 1.09s (100 records)\n",
      "‚úÖ Batch 2 inserted in 0.25s (100 records)\n",
      "‚úÖ Batch 2 inserted in 0.24s (100 records)\n",
      "‚úÖ Batch 2 inserted in 0.27s (100 records)\n",
      "‚úÖ Batch 2 inserted in 0.30s (100 records)\n",
      "‚úÖ Batch 3 inserted in 0.07s (43 records)\n",
      "‚úÖ Batch 3 inserted in 0.18s (100 records)\n",
      "‚úÖ Batch 3 inserted in 0.19s (100 records)\n",
      "üéâ All batches uploaded in 1.56 seconds.\n"
     ]
    }
   ],
   "source": [
    "#insert soc codes data\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timezone\n",
    "import boto3\n",
    "from queue import Queue\n",
    "\n",
    "# CONFIGURATION\n",
    "CSV_FILE = \"Datasets/unique_occ_codes.csv\"\n",
    "REGION = \"us-west-2\"\n",
    "BATCH_SIZE = 100\n",
    "NUM_THREADS = 4\n",
    "REJECTED_FILE = \"rejected_soc_codes.csv\"\n",
    "\n",
    "# AWS setup\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=REGION)\n",
    "table = dynamodb.Table(SOC_CODES_TABLE)\n",
    "\n",
    "queue = Queue()\n",
    "rejected_rows = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def current_timestamp():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def process_batch(batch, batch_number):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        with table.batch_writer(overwrite_by_pkeys=['occCode']) as writer:\n",
    "            for item in batch:\n",
    "                try:\n",
    "                    writer.put_item(Item=item)\n",
    "                except Exception as e:\n",
    "                    with lock:\n",
    "                        rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚úÖ Batch {batch_number} inserted in {time.time() - start:.2f}s ({len(batch)} records)\")\n",
    "    except Exception as e:\n",
    "        with lock:\n",
    "            for item in batch:\n",
    "                rejected_rows.append({**item, 'error': str(e)})\n",
    "        print(f\"‚ùå Batch {batch_number} failed with error: {str(e)}\")\n",
    "\n",
    "def worker():\n",
    "    batch_number = 1\n",
    "    while True:\n",
    "        batch = queue.get()\n",
    "        if batch is None:\n",
    "            break\n",
    "        process_batch(batch, batch_number)\n",
    "        batch_number += 1\n",
    "        queue.task_done()\n",
    "\n",
    "def read_and_queue_csv(filepath):\n",
    "    with open(filepath, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        batch = []\n",
    "        for row_num, row in enumerate(reader, 2):\n",
    "            try:\n",
    "                now = current_timestamp()\n",
    "                item = {\n",
    "                    'occCode': str(row['OCC_CODE']),\n",
    "                    'occTitle': str(row['OCC_TITLE']),\n",
    "                    'createdAt': now,\n",
    "                    'updatedAt': now,\n",
    "                }\n",
    "                batch.append(item)\n",
    "                if len(batch) == BATCH_SIZE:\n",
    "                    queue.put(batch)\n",
    "                    batch = []\n",
    "                if row_num % 1000 == 0:\n",
    "                    print(f\"   ...Processed {row_num} rows so far\")\n",
    "            except Exception as e:\n",
    "                with lock:\n",
    "                    rejected_rows.append({**row, 'error': str(e)})\n",
    "        if batch:\n",
    "            queue.put(batch)\n",
    "\n",
    "def save_rejected_rows():\n",
    "    if rejected_rows:\n",
    "        keys = rejected_rows[0].keys()\n",
    "        with open(REJECTED_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rejected_rows)\n",
    "        print(f\"‚ö†Ô∏è {len(rejected_rows)} records rejected. Saved to {REJECTED_FILE}\")\n",
    "\n",
    "# Main\n",
    "print(f\"üöÄ Starting upload from: {CSV_FILE}\")\n",
    "start = time.time()\n",
    "\n",
    "threads = []\n",
    "for _ in range(NUM_THREADS):\n",
    "    t = threading.Thread(target=worker)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "read_and_queue_csv(CSV_FILE)\n",
    "queue.join()\n",
    "\n",
    "# Stop workers\n",
    "for _ in range(NUM_THREADS):\n",
    "    queue.put(None)\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "save_rejected_rows()\n",
    "print(f\"üéâ All batches uploaded in {time.time() - start:.2f} seconds.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
