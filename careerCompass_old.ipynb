{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m year = \u001b[38;5;28mnext\u001b[39m((part \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m filename.split(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m part.isdigit()), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Read the Excel file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Convert column names to lowercase\u001b[39;00m\n\u001b[32m     32\u001b[39m df.columns = [col.lower() \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df.columns]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/io/excel/_base.py:508\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     data = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/io/excel/_base.py:1616\u001b[39m, in \u001b[36mExcelFile.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\n\u001b[32m   1577\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1578\u001b[39m     sheet_name: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1596\u001b[39m     **kwds,\n\u001b[32m   1597\u001b[39m ) -> DataFrame | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[32m   1598\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1599\u001b[39m \u001b[33;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[32m   1600\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1614\u001b[39m \u001b[33;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[32m   1615\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1617\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1635\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/io/excel/_base.py:778\u001b[39m, in \u001b[36mBaseExcelReader.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m    775\u001b[39m     sheet = \u001b[38;5;28mself\u001b[39m.get_sheet_by_index(asheetname)\n\u001b[32m    777\u001b[39m file_rows_needed = \u001b[38;5;28mself\u001b[39m._calc_rows(header, index_col, skiprows, nrows)\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_sheet_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_rows_needed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sheet, \u001b[33m\"\u001b[39m\u001b[33mclose\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    780\u001b[39m     \u001b[38;5;66;03m# pyxlsb opens two TemporaryFiles\u001b[39;00m\n\u001b[32m    781\u001b[39m     sheet.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pandas/io/excel/_openpyxl.py:615\u001b[39m, in \u001b[36mOpenpyxlReader.get_sheet_data\u001b[39m\u001b[34m(self, sheet, file_rows_needed)\u001b[39m\n\u001b[32m    613\u001b[39m data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar]] = []\n\u001b[32m    614\u001b[39m last_row_with_data = -\u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# trim trailing empty elements\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/openpyxl/worksheet/_read_only.py:85\u001b[39m, in \u001b[36mReadOnlyWorksheet._cells_by_row\u001b[39m\u001b[34m(self, min_col, min_row, max_col, max_row, values_only)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_source() \u001b[38;5;28;01mas\u001b[39;00m src:\n\u001b[32m     78\u001b[39m     parser = WorkSheetParser(src,\n\u001b[32m     79\u001b[39m                              \u001b[38;5;28mself\u001b[39m._shared_strings,\n\u001b[32m     80\u001b[39m                              data_only=\u001b[38;5;28mself\u001b[39m.parent.data_only,\n\u001b[32m     81\u001b[39m                              epoch=\u001b[38;5;28mself\u001b[39m.parent.epoch,\n\u001b[32m     82\u001b[39m                              date_formats=\u001b[38;5;28mself\u001b[39m.parent._date_formats,\n\u001b[32m     83\u001b[39m                              timedelta_formats=\u001b[38;5;28mself\u001b[39m.parent._timedelta_formats)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_row\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_row\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:166\u001b[39m, in \u001b[36mWorkSheetParser.parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, prop[\u001b[32m0\u001b[39m], obj)\n\u001b[32m    165\u001b[39m     element.clear()\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m tag_name == ROW_TAG:\n\u001b[32m    167\u001b[39m     row = \u001b[38;5;28mself\u001b[39m.parse_row(element)\n\u001b[32m    168\u001b[39m     element.clear()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Converts the salary data from Excel files to CSV format for the next script\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = \"Datasets/salary\"\n",
    "output_folder = \"Datasets/Salaries\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Required columns in lowercase\n",
    "required_columns = [\n",
    "    \"occ_code\", \"occ_title\", \"area_title\",\n",
    "    \"naics_title\", \"a_median\", \"a_pct10\", \"a_pct90\"\n",
    "]\n",
    "\n",
    "# Get all Excel files from the folder\n",
    "excel_files = glob(os.path.join(input_folder, \"*.xlsx\"))\n",
    "\n",
    "# Process each file\n",
    "for file_path in excel_files:\n",
    "    try:\n",
    "        filename = os.path.basename(file_path)\n",
    "        year = next((part for part in filename.split('_') if part.isdigit()), None)\n",
    "\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "\n",
    "        # Convert column names to lowercase\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "        # Add year column\n",
    "        df[\"year\"] = year\n",
    "\n",
    "        # Reorder and filter columns (now all lowercase)\n",
    "        filtered_df = df[required_columns + [\"year\"]]\n",
    "\n",
    "        # Write to CSV\n",
    "        output_path = os.path.join(output_folder, f\"filtered_occupation_data_{year}.csv\")\n",
    "        filtered_df.to_csv(output_path, index=False)\n",
    "\n",
    "        # Uncomment for debug:\n",
    "        print(f\"✅ Processed and saved: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found salary files: ['filtered_occupation_data_2016.csv', 'filtered_occupation_data_2017.csv', 'filtered_occupation_data_2018.csv', 'filtered_occupation_data_2019.csv', 'filtered_occupation_data_2020.csv', 'filtered_occupation_data_2021.csv', 'filtered_occupation_data_2022.csv', 'filtered_occupation_data_2023.csv', 'filtered_occupation_data_2024.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/4p7wb30d22l5wb72_xjjd0xh0000gn/T/ipykernel_33751/1644696939.py:246: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DynamoDB-ready CSV written to 'dynamodb_ready_soc_output.csv' with 1550 records.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# import csv\n",
    "# import os\n",
    "# from glob import glob\n",
    "\n",
    "# # Directory setup\n",
    "# dataset_dir = \"Datasets/\"\n",
    "# salary_dir = f\"{dataset_dir}Salaries/\"\n",
    "# salary_files = sorted(glob(os.path.join(salary_dir, \"filtered_occupation_data_*.csv\")))\n",
    "\n",
    "# # Check salary files\n",
    "# if not salary_files:\n",
    "#     raise FileNotFoundError(\"❌ No salary_data_*.csv files found.\")\n",
    "\n",
    "# print(f\"✅ Found salary files: {[os.path.basename(f) for f in salary_files]}\")\n",
    "\n",
    "# # Load all salary CSVs\n",
    "# salary_dfs = []\n",
    "# for file in salary_files:\n",
    "#     try:\n",
    "#         df = pd.read_csv(file)\n",
    "#         df[\"YEAR\"] = int(file.split(\"_\")[-1].split(\".\")[0])\n",
    "#         salary_dfs.append(df)\n",
    "#     except Exception as e:\n",
    "#         print(f\"⚠️ Error reading {file}: {e}\")\n",
    "\n",
    "# salary_df = pd.concat(salary_dfs, ignore_index=True)\n",
    "\n",
    "# # Load education, skills, and descriptions\n",
    "# education_df = pd.read_csv(os.path.join(dataset_dir, \"education_data.csv\"))\n",
    "# skills_df = pd.read_csv(os.path.join(dataset_dir, \"skills_data.csv\"))\n",
    "# description_df = pd.read_csv(os.path.join(dataset_dir, \"description.csv\"))\n",
    "# description_df[\"Code\"] = description_df[\"Code\"].astype(str)\n",
    "# description_map = description_df.set_index(\"Code\")[\"Description\"].to_dict()\n",
    "\n",
    "# # Ensure numeric conversion\n",
    "# salary_df[\"a_pct10\"] = pd.to_numeric(salary_df[\"a_pct10\"], errors=\"coerce\")\n",
    "# salary_df[\"a_median\"] = pd.to_numeric(salary_df[\"a_median\"], errors=\"coerce\")\n",
    "# salary_df[\"a_pct90\"] = pd.to_numeric(salary_df[\"a_pct90\"], errors=\"coerce\")\n",
    "\n",
    "# # Monthly salaries\n",
    "# salary_df[\"M_PCT10\"] = (salary_df[\"a_pct10\"] / 12).round(2)\n",
    "# salary_df[\"M_MEDIAN\"] = (salary_df[\"a_median\"] / 12).round(2)\n",
    "# salary_df[\"M_PCT90\"] = (salary_df[\"a_pct90\"] / 12).round(2)\n",
    "\n",
    "# # Education mapping\n",
    "# edu_map = {\n",
    "#     \"Less_than_hs\": \"LESS_THAN_HS\",\n",
    "#     \"hs_or_eq\": \"HIGH_SCHOOL\",\n",
    "#     \"Associate_degree\": \"ASSOCIATE\",\n",
    "#     \"Bachelor_degree\": \"BACHELOR\",\n",
    "#     \"Master_degree\": \"MASTERS\",\n",
    "#     \"Doctorate_degree\": \"DOCTORATE\",\n",
    "#     \"No_requirement\": \"NO_REQ\",\n",
    "#     \"Professional_degree\": \"PROFESSIONAL\"\n",
    "# }\n",
    "\n",
    "# # Build records\n",
    "# records = []\n",
    "# grouped_salary = salary_df.groupby([\"occ_code\", \"year\"])\n",
    "\n",
    "# for (soc_code, year), group in grouped_salary:\n",
    "#     existing_record = next((r for r in records if r[\"soc_code\"] == soc_code), None)\n",
    "\n",
    "#     if not existing_record:\n",
    "#         desc = description_map.get(soc_code, \"\")\n",
    "#         existing_record = {\n",
    "#             \"soc_code\": soc_code,\n",
    "#             \"title\": group.iloc[0][\"occ_title\"],\n",
    "#             \"description\": description_map.get(soc_code, \"\"),\n",
    "#             \"salary\": {},\n",
    "#             \"education\": {key: \"\" for key in edu_map.values()},\n",
    "#             \"typicalSkills\": []\n",
    "#         }\n",
    "#         records.append(existing_record)\n",
    "\n",
    "#     year_str = str(year)\n",
    "#     existing_record[\"salary\"].setdefault(year_str, {})\n",
    "\n",
    "#     for _, row in group.iterrows():\n",
    "#         state = row[\"area_title\"]\n",
    "#         industry = row[\"naics_title\"]\n",
    "#         existing_record[\"salary\"][year_str].setdefault(state, {})[industry] = {\n",
    "#             \"A_MEDIAN\": float(row[\"a_median\"]),\n",
    "#             \"M_PCT10\": float(row[\"M_PCT10\"]),\n",
    "#             \"M_MEDIAN\": float(row[\"M_MEDIAN\"]),\n",
    "#             \"M_PCT90\": float(row[\"M_PCT90\"])\n",
    "#         }\n",
    "\n",
    "# # Add education data\n",
    "# for record in records:\n",
    "#     soc_code = record[\"soc_code\"]\n",
    "#     edu_rows = education_df[(education_df[\"SOC\"] == soc_code) | (education_df[\"SOC\"] == \"00-0000\")]\n",
    "#     for _, edu_row in edu_rows.iterrows():\n",
    "#         est_code = edu_row[\"ESTIMATECODE\"]\n",
    "#         est_value = str(edu_row[\"ESTIMATE\"])\n",
    "#         if est_code in edu_map:\n",
    "#             record[\"education\"][edu_map[est_code]] = est_value\n",
    "\n",
    "# # Add skills data\n",
    "# for record in records:\n",
    "#     soc_code = record[\"soc_code\"]\n",
    "#     skills_row = skills_df[skills_df[\"SOC_CODE\"] == soc_code]\n",
    "#     if not skills_row.empty:\n",
    "#         raw_skills = skills_row.iloc[0][\"TYPICAL_SKILLS\"].replace(\"'\", \"\\\"\")\n",
    "#         try:\n",
    "#             skills_list = json.loads(raw_skills)\n",
    "#             record[\"typicalSkills\"] = sorted(set(skills_list))\n",
    "#         except json.JSONDecodeError:\n",
    "#             print(f\"⚠️ Error decoding skills for SOC {soc_code}\")\n",
    "#             record[\"typicalSkills\"] = []\n",
    "\n",
    "# # Write to output CSV\n",
    "# output_file = \"soc_compiled_output.csv\"\n",
    "# with open(output_file, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "#     fieldnames = [\"soc_code\", \"title\", \"description\", \"salary\", \"education\", \"typicalSkills\"]\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#     writer.writeheader()\n",
    "#     for rec in records:\n",
    "#         writer.writerow({\n",
    "#             \"soc_code\": rec[\"soc_code\"],\n",
    "#             \"title\": rec[\"title\"],\n",
    "#             \"description\": rec[\"description\"],\n",
    "#             \"salary\": json.dumps(rec[\"salary\"]),\n",
    "#             \"education\": json.dumps(rec[\"education\"]),\n",
    "#             \"typicalSkills\": json.dumps(rec[\"typicalSkills\"])\n",
    "#         })\n",
    "\n",
    "# print(f\"✅ Done! Output written to '{output_file}' with {len(records)} SOC records.\")\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# Directory setup\n",
    "dataset_dir = \"Datasets/\"\n",
    "salary_dir = f\"{dataset_dir}Salaries/\"\n",
    "salary_files = sorted(glob(os.path.join(salary_dir, \"filtered_occupation_data_*.csv\")))\n",
    "\n",
    "# Check salary files\n",
    "if not salary_files:\n",
    "    raise FileNotFoundError(\"❌ No salary_data_*.csv files found.\")\n",
    "\n",
    "print(f\"✅ Found salary files: {[os.path.basename(f) for f in salary_files]}\")\n",
    "\n",
    "# Load all salary CSVs\n",
    "salary_dfs = []\n",
    "for file in salary_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        df[\"YEAR\"] = int(file.split(\"_\")[-1].split(\".\")[0])\n",
    "        salary_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {file}: {e}\")\n",
    "\n",
    "salary_df = pd.concat(salary_dfs, ignore_index=True)\n",
    "\n",
    "# Load education, skills, and descriptions\n",
    "education_df = pd.read_csv(os.path.join(dataset_dir, \"education_data.csv\"))\n",
    "skills_df = pd.read_csv(os.path.join(dataset_dir, \"skills_data.csv\"))\n",
    "description_df = pd.read_csv(os.path.join(dataset_dir, \"description.csv\"))\n",
    "description_df[\"Code\"] = description_df[\"Code\"].astype(str)\n",
    "description_map = description_df.set_index(\"Code\")[\"Description\"].to_dict()\n",
    "\n",
    "# Ensure numeric conversion\n",
    "salary_df[\"a_pct10\"] = pd.to_numeric(salary_df[\"a_pct10\"], errors=\"coerce\")\n",
    "salary_df[\"a_median\"] = pd.to_numeric(salary_df[\"a_median\"], errors=\"coerce\")\n",
    "salary_df[\"a_pct90\"] = pd.to_numeric(salary_df[\"a_pct90\"], errors=\"coerce\")\n",
    "\n",
    "# Monthly salaries\n",
    "salary_df[\"M_PCT10\"] = (salary_df[\"a_pct10\"] / 12).round(2)\n",
    "salary_df[\"M_MEDIAN\"] = (salary_df[\"a_median\"] / 12).round(2)\n",
    "salary_df[\"M_PCT90\"] = (salary_df[\"a_pct90\"] / 12).round(2)\n",
    "\n",
    "# Education mapping\n",
    "edu_map = {\n",
    "    \"Less_than_hs\": \"LESS_THAN_HS\",\n",
    "    \"hs_or_eq\": \"HIGH_SCHOOL\",\n",
    "    \"Associate_degree\": \"ASSOCIATE\",\n",
    "    \"Bachelor_degree\": \"BACHELOR\",\n",
    "    \"Master_degree\": \"MASTERS\",\n",
    "    \"Doctorate_degree\": \"DOCTORATE\",\n",
    "    \"No_requirement\": \"NO_REQ\",\n",
    "    \"Professional_degree\": \"PROFESSIONAL\"\n",
    "}\n",
    "\n",
    "# Build records\n",
    "records = []\n",
    "grouped_salary = salary_df.groupby([\"occ_code\", \"year\"])\n",
    "\n",
    "for (soc_code, year), group in grouped_salary:\n",
    "    existing_record = next((r for r in records if r[\"soc_code\"] == soc_code), None)\n",
    "\n",
    "    if not existing_record:\n",
    "        existing_record = {\n",
    "            \"soc_code\": soc_code,\n",
    "            \"title\": group.iloc[0][\"occ_title\"],\n",
    "            \"description\": description_map.get(soc_code, \"\"),\n",
    "            \"salary\": {},\n",
    "            \"education\": {key: \"\" for key in edu_map.values()},\n",
    "            \"typicalSkills\": []\n",
    "        }\n",
    "        records.append(existing_record)\n",
    "\n",
    "    year_str = str(year)\n",
    "    existing_record[\"salary\"].setdefault(year_str, {})\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        state = row[\"area_title\"]\n",
    "        industry = row[\"naics_title\"]\n",
    "        existing_record[\"salary\"][year_str].setdefault(state, {})[industry] = {\n",
    "            \"A_MEDIAN\": float(row[\"a_median\"]),\n",
    "            \"M_PCT10\": float(row[\"M_PCT10\"]),\n",
    "            \"M_MEDIAN\": float(row[\"M_MEDIAN\"]),\n",
    "            \"M_PCT90\": float(row[\"M_PCT90\"])\n",
    "        }\n",
    "\n",
    "# Add education data\n",
    "for record in records:\n",
    "    soc_code = record[\"soc_code\"]\n",
    "    edu_rows = education_df[(education_df[\"SOC\"] == soc_code) | (education_df[\"SOC\"] == \"00-0000\")]\n",
    "    for _, edu_row in edu_rows.iterrows():\n",
    "        est_code = edu_row[\"ESTIMATECODE\"]\n",
    "        est_value = str(edu_row[\"ESTIMATE\"])\n",
    "        if est_code in edu_map:\n",
    "            record[\"education\"][edu_map[est_code]] = est_value\n",
    "\n",
    "# Add skills data\n",
    "for record in records:\n",
    "    soc_code = record[\"soc_code\"]\n",
    "    skills_row = skills_df[skills_df[\"SOC_CODE\"] == soc_code]\n",
    "    if not skills_row.empty:\n",
    "        raw_skills = skills_row.iloc[0][\"TYPICAL_SKILLS\"].replace(\"'\", \"\\\"\")\n",
    "        try:\n",
    "            skills_list = json.loads(raw_skills)\n",
    "            record[\"typicalSkills\"] = sorted(set(skills_list))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"⚠️ Error decoding skills for SOC {soc_code}\")\n",
    "            record[\"typicalSkills\"] = []\n",
    "\n",
    "# Add timestamps\n",
    "timestamp = datetime.utcnow().isoformat()\n",
    "\n",
    "# Write to DynamoDB-ready CSV\n",
    "output_file = \"dynamodb_ready_soc_output.csv\"\n",
    "with open(output_file, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\n",
    "        \"soc_code\",       # Partition key\n",
    "        \"title\",\n",
    "        \"description\",\n",
    "        \"salary\",         # JSON string\n",
    "        \"education\",      # JSON string\n",
    "        \"typicalSkills\",  # JSON string\n",
    "        \"createdAt\",\n",
    "        \"updatedAt\"\n",
    "    ]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for rec in records:\n",
    "        writer.writerow({\n",
    "            \"soc_code\": rec[\"soc_code\"],\n",
    "            \"title\": rec[\"title\"],\n",
    "            \"description\": rec[\"description\"],\n",
    "            \"salary\": json.dumps(rec[\"salary\"], ensure_ascii=False),\n",
    "            \"education\": json.dumps(rec[\"education\"], ensure_ascii=False),\n",
    "            \"typicalSkills\": json.dumps(rec[\"typicalSkills\"], ensure_ascii=False),\n",
    "            \"createdAt\": timestamp,\n",
    "            \"updatedAt\": timestamp\n",
    "        })\n",
    "\n",
    "print(f\"✅ DynamoDB-ready CSV written to '{output_file}' with {len(records)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to parse JSON object: {\"2016\": {\"U.S.\": {\"Cross-industry\": {\"A_MEDIAN\": 53640.0, \"M_PCT10\": 2530.83, \"M_MEDIAN\": 4470.0, \"... — Invalid control character at: line 1 column 407243 (char 407242)\n",
      "✅ Cleaned CSV written to: dynamodb_ready_output.csv\n"
     ]
    }
   ],
   "source": [
    "# # Cleans up the CSV file generated\n",
    "# import csv\n",
    "# import json\n",
    "# import sys\n",
    "\n",
    "# # ✅ Raise max CSV field size limit (handles very large salary fields)\n",
    "# csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# # ✅ File paths\n",
    "# INPUT_CSV = \"soc_compiled_output.csv\"\n",
    "# OUTPUT_CSV = \"dynamodb_ready_output.csv\"\n",
    "# OUTPUT_JSON = \"dynamodb_ready_output.json\"\n",
    "\n",
    "# # ✅ Utility functions\n",
    "# def clean_json_field(raw_value):\n",
    "#     try:\n",
    "#         if raw_value.startswith('\"') and raw_value.endswith('\"'):\n",
    "#             raw_value = raw_value[1:-1]\n",
    "#         return json.loads(raw_value.replace('\\\\\"', '\"'))\n",
    "#     except Exception as e:\n",
    "#         print(f\"⚠️ Failed to parse JSON object: {raw_value[:100]}... — {e}\")\n",
    "#         return {}\n",
    "\n",
    "# def clean_array_field(raw_value):\n",
    "#     try:\n",
    "#         if raw_value.startswith('\"') and raw_value.endswith('\"'):\n",
    "#             raw_value = raw_value[1:-1]\n",
    "#         return json.loads(raw_value.replace('\\\\\"', '\"'))\n",
    "#     except Exception as e:\n",
    "#         print(f\"⚠️ Failed to parse JSON array: {raw_value[:100]}... — {e}\")\n",
    "#         return []\n",
    "\n",
    "# # ✅ Clean the data\n",
    "# cleaned_rows = []\n",
    "\n",
    "# with open(INPUT_CSV, \"r\", encoding=\"utf-8\") as infile:\n",
    "#     reader = csv.DictReader(infile)\n",
    "#     for row in reader:\n",
    "#         row[\"education\"] = clean_json_field(row[\"education\"])\n",
    "#         row[\"salary\"] = clean_json_field(row[\"salary\"])\n",
    "#         row[\"typicalSkills\"] = clean_array_field(row[\"typicalSkills\"])\n",
    "#         cleaned_rows.append(row)\n",
    "\n",
    "# # ✅ Write cleaned CSV\n",
    "# with open(OUTPUT_CSV, \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
    "#     fieldnames = cleaned_rows[0].keys()\n",
    "#     writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "#     writer.writeheader()\n",
    "#     for row in cleaned_rows:\n",
    "#         row[\"education\"] = json.dumps(row[\"education\"], ensure_ascii=False)\n",
    "#         row[\"salary\"] = json.dumps(row[\"salary\"], ensure_ascii=False)\n",
    "#         row[\"typicalSkills\"] = json.dumps(row[\"typicalSkills\"], ensure_ascii=False)\n",
    "#         writer.writerow(row)\n",
    "\n",
    "# print(f\"✅ Cleaned CSV written to: {OUTPUT_CSV}\")\n",
    "\n",
    "# # # ✅ Write cleaned JSON\n",
    "# # with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as jsonfile:\n",
    "# #     json.dump(cleaned_rows, jsonfile, indent=2, ensure_ascii=False)\n",
    "\n",
    "# # print(f\"✅ DynamoDB-ready JSON written to: {OUTPUT_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned file written to: dynamodb_ready_output_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "# import json\n",
    "# import sys\n",
    "\n",
    "# csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# INPUT_CSV = \"dynamodb_ready_output.csv\"\n",
    "# OUTPUT_CSV = \"dynamodb_ready_output_fixed.csv\"\n",
    "\n",
    "# def clean_json_field(raw_value):\n",
    "#     try:\n",
    "#         if raw_value.startswith('\"') and raw_value.endswith('\"'):\n",
    "#             raw_value = raw_value[1:-1]\n",
    "#         return json.loads(raw_value.replace('\\\\\"', '\"'))\n",
    "#     except Exception as e:\n",
    "#         print(f\"⚠️ Failed to parse JSON object: {raw_value[:100]}... — {e}\")\n",
    "#         return {}\n",
    "\n",
    "# def clean_array_field(raw_value):\n",
    "#     try:\n",
    "#         if raw_value.startswith('\"') and raw_value.endswith('\"'):\n",
    "#             raw_value = raw_value[1:-1]\n",
    "#         return json.loads(raw_value.replace('\\\\\"', '\"'))\n",
    "#     except Exception as e:\n",
    "#         print(f\"⚠️ Failed to parse JSON array: {raw_value[:100]}... — {e}\")\n",
    "#         return []\n",
    "\n",
    "# cleaned_rows = []\n",
    "\n",
    "# with open(INPUT_CSV, \"r\", encoding=\"utf-8\") as infile:\n",
    "#     reader = csv.DictReader(infile)\n",
    "#     for row in reader:\n",
    "#         row[\"education\"] = clean_json_field(row.get(\"education\", \"\"))\n",
    "#         row[\"salary\"] = clean_json_field(row.get(\"salary\", \"\"))\n",
    "#         row[\"typicalSkills\"] = clean_array_field(row.get(\"typicalSkills\", \"[]\"))\n",
    "#         cleaned_rows.append(row)\n",
    "\n",
    "# with open(OUTPUT_CSV, \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
    "#     fieldnames = cleaned_rows[0].keys()\n",
    "#     writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "#     writer.writeheader()\n",
    "#     for row in cleaned_rows:\n",
    "#         row[\"education\"] = json.dumps(row[\"education\"], ensure_ascii=False)\n",
    "#         row[\"salary\"] = json.dumps(row[\"salary\"], ensure_ascii=False)\n",
    "#         row[\"typicalSkills\"] = json.dumps(row[\"typicalSkills\"], ensure_ascii=False)\n",
    "#         writer.writerow(row)\n",
    "\n",
    "# print(f\"✅ Cleaned file written to: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "field larger than field limit (131072)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mError\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43mupload_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSV_FILE_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mupload_csv\u001b[39m\u001b[34m(filepath)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, newline=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[32m     57\u001b[39m     reader = csv.DictReader(csvfile)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43mupload_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Write rejected rows if any\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/csv.py:116\u001b[39m, in \u001b[36mDictReader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.line_num == \u001b[32m0\u001b[39m:\n\u001b[32m    114\u001b[39m     \u001b[38;5;66;03m# Used only for its side effect.\u001b[39;00m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28mself\u001b[39m.fieldnames\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m row = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;28mself\u001b[39m.line_num = \u001b[38;5;28mself\u001b[39m.reader.line_num\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# unlike the basic reader, we prefer not to return blanks,\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# because we will typically wind up with a dict full of None\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# values\u001b[39;00m\n",
      "\u001b[31mError\u001b[39m: field larger than field limit (131072)"
     ]
    }
   ],
   "source": [
    "#Inserts the csv file into the DynamoDB database\n",
    "import csv\n",
    "import json\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "REGION = 'us-west-2'\n",
    "#Chetna Table name: careerData-eh7gingt5zao7c7znywp6lvh7q-NONE\n",
    "#Gary Table name: careerData-5oaexymterhv5cq3utxzzbxrqu-NONE\n",
    "#Trupti Table name: careerData-mnwl2lvfqzhuzcdsnpnfazqvfm-NONE\n",
    "TABLE_NAME = 'careerData-mnwl2lvfqzhuzcdsnpnfazqvfm-NONE'  \n",
    "CSV_FILE_PATH = 'dynamodb_ready_soc_output.csv'\n",
    "REJECTED_FILE_PATH = f'rejected-{CSV_FILE_PATH}'\n",
    "\n",
    "# DynamoDB client\n",
    "dynamodb = boto3.resource('dynamodb', region_name=REGION)\n",
    "table = dynamodb.Table(TABLE_NAME)\n",
    "\n",
    "# Tracking\n",
    "total_uploaded = 0\n",
    "total_rejected = 0\n",
    "rejected_rows = []\n",
    "\n",
    "# Upload a single record\n",
    "def upload_record(row):\n",
    "    global total_uploaded, total_rejected\n",
    "\n",
    "    try:\n",
    "        education = json.loads(row['education'] or '{}')\n",
    "        salary = json.loads(row['salary'] or '{}')\n",
    "    except Exception as e:\n",
    "        rejected_rows.append(row)\n",
    "        total_rejected += 1\n",
    "        return\n",
    "\n",
    "    item = {\n",
    "        'occ_code': row['soc_code'],\n",
    "        'occ_title': row['title'],\n",
    "        'description': row.get('description', ''),\n",
    "        'education': education,\n",
    "        'salary': salary,\n",
    "        'createdAt': datetime.utcnow().isoformat(),\n",
    "        'updatedAt': datetime.utcnow().isoformat(),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        table.put_item(Item=item)\n",
    "        total_uploaded += 1\n",
    "    except Exception as e:\n",
    "        rejected_rows.append(row)\n",
    "        total_rejected += 1\n",
    "\n",
    "# Main CSV upload process\n",
    "def upload_csv(filepath):\n",
    "    with open(filepath, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            upload_record(row)\n",
    "\n",
    "    # Write rejected rows if any\n",
    "    if rejected_rows:\n",
    "        with open(REJECTED_FILE_PATH, 'w', newline='', encoding='utf-8') as reject_file:\n",
    "            writer = csv.DictWriter(reject_file, fieldnames=rejected_rows[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rejected_rows)\n",
    "        print(f\"⚠️ {total_rejected} records rejected. Saved to {REJECTED_FILE_PATH}\")\n",
    "\n",
    "    print(f\"✅ Upload complete. {total_uploaded} records uploaded, {total_rejected} rejected.\")\n",
    "\n",
    "# Start the process\n",
    "if __name__ == '__main__':\n",
    "    upload_csv(CSV_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found salary files: ['filtered_occupation_data_2016.csv', 'filtered_occupation_data_2017.csv', 'filtered_occupation_data_2018.csv', 'filtered_occupation_data_2019.csv', 'filtered_occupation_data_2020.csv', 'filtered_occupation_data_2021.csv', 'filtered_occupation_data_2022.csv', 'filtered_occupation_data_2023.csv', 'filtered_occupation_data_2024.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/4p7wb30d22l5wb72_xjjd0xh0000gn/T/ipykernel_33751/1590782623.py:120: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DynamoDB-ready CSV written to 'dynamodb_ready_soc_output.csv' with 1550 records.\n",
      "☁️ Uploading 1550 records to DynamoDB table 'careerData-mnwl2lvfqzhuzcdsnpnfazqvfm-NONE'...\n",
      "❌ Failed to insert item 11-3020: An error occurred (413) when calling the BatchWriteItem operation: \n",
      "❌ Failed to insert item 11-9039: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 11-9171: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 13-1081: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 13-2041: An error occurred (413) when calling the BatchWriteItem operation: \n",
      "❌ Failed to insert item 15-1131: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 15-1243: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 15-2091: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 17-2071: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 17-3013: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 19-1029: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 19-3010: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 19-4020: An error occurred (ValidationException) when calling the BatchWriteItem operation: The provided key element does not match the schema\n",
      "❌ Failed to insert item 21-0000: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 21-2021: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 25-1022: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 25-1080: An error occurred (ValidationException) when calling the BatchWriteItem operation: The provided key element does not match the schema\n",
      "❌ Failed to insert item 25-2021: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 25-3098: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 27-0000: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 27-2030: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 27-3092: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 29-1024: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 29-1125: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 29-1216: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 29-2021: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 29-2080: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 31-1015: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 31-9099: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 33-3040: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 35-2000: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 35-9021: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 39-1010: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 39-3092: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 39-7010: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 41-2020: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 41-9020: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 43-3021: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 43-4061: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 43-5011: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 43-6014: An error occurred (413) when calling the BatchWriteItem operation: \n",
      "❌ Failed to insert item 45-1010: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 47-0000: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 47-2073: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 47-2220: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 47-4061: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 47-5051: An error occurred (ValidationException) when calling the BatchWriteItem operation: The provided key element does not match the schema\n",
      "❌ Failed to insert item 49-2094: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 49-3093: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 49-9080: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 51-2031: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 51-4000: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 51-4080: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 51-6040: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 51-7041: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 51-9023: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 51-9150: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 53-1047: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 53-3052: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 53-5022: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 53-7030: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "❌ Failed to insert item 53-7199: An error occurred (ValidationException) when calling the BatchWriteItem operation: Item size has exceeded the maximum allowed size\n",
      "✅ Upload complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime, timezone\n",
    "import boto3\n",
    "from botocore.exceptions import BotoCoreError, ClientError\n",
    "\n",
    "# DynamoDB Table Name\n",
    "TABLE_NAME = 'careerData-mnwl2lvfqzhuzcdsnpnfazqvfm-NONE'\n",
    "\n",
    "# Directory setup\n",
    "dataset_dir = \"Datasets/\"\n",
    "salary_dir = f\"{dataset_dir}Salaries/\"\n",
    "salary_files = sorted(glob(os.path.join(salary_dir, \"filtered_occupation_data_*.csv\")))\n",
    "\n",
    "# Check salary files\n",
    "if not salary_files:\n",
    "    raise FileNotFoundError(\"❌ No salary_data_*.csv files found.\")\n",
    "\n",
    "print(f\"✅ Found salary files: {[os.path.basename(f) for f in salary_files]}\")\n",
    "\n",
    "# Load all salary CSVs\n",
    "salary_dfs = []\n",
    "for file in salary_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        df[\"YEAR\"] = int(file.split(\"_\")[-1].split(\".\")[0])\n",
    "        salary_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {file}: {e}\")\n",
    "\n",
    "salary_df = pd.concat(salary_dfs, ignore_index=True)\n",
    "\n",
    "# Load education, skills, and descriptions\n",
    "education_df = pd.read_csv(os.path.join(dataset_dir, \"education_data.csv\"))\n",
    "skills_df = pd.read_csv(os.path.join(dataset_dir, \"skills_data.csv\"))\n",
    "description_df = pd.read_csv(os.path.join(dataset_dir, \"description.csv\"))\n",
    "description_df[\"Code\"] = description_df[\"Code\"].astype(str)\n",
    "description_map = description_df.set_index(\"Code\")[\"Description\"].to_dict()\n",
    "\n",
    "# Ensure numeric conversion\n",
    "salary_df[\"a_pct10\"] = pd.to_numeric(salary_df[\"a_pct10\"], errors=\"coerce\")\n",
    "salary_df[\"a_median\"] = pd.to_numeric(salary_df[\"a_median\"], errors=\"coerce\")\n",
    "salary_df[\"a_pct90\"] = pd.to_numeric(salary_df[\"a_pct90\"], errors=\"coerce\")\n",
    "\n",
    "# Monthly salaries\n",
    "salary_df[\"M_PCT10\"] = (salary_df[\"a_pct10\"] / 12).round(2)\n",
    "salary_df[\"M_MEDIAN\"] = (salary_df[\"a_median\"] / 12).round(2)\n",
    "salary_df[\"M_PCT90\"] = (salary_df[\"a_pct90\"] / 12).round(2)\n",
    "\n",
    "# Education mapping\n",
    "edu_map = {\n",
    "    \"Less_than_hs\": \"LESS_THAN_HS\",\n",
    "    \"hs_or_eq\": \"HIGH_SCHOOL\",\n",
    "    \"Associate_degree\": \"ASSOCIATE\",\n",
    "    \"Bachelor_degree\": \"BACHELOR\",\n",
    "    \"Master_degree\": \"MASTERS\",\n",
    "    \"Doctorate_degree\": \"DOCTORATE\",\n",
    "    \"No_requirement\": \"NO_REQ\",\n",
    "    \"Professional_degree\": \"PROFESSIONAL\"\n",
    "}\n",
    "\n",
    "# Build records\n",
    "records = []\n",
    "grouped_salary = salary_df.groupby([\"occ_code\", \"year\"])\n",
    "\n",
    "for (soc_code, year), group in grouped_salary:\n",
    "    existing_record = next((r for r in records if r[\"soc_code\"] == soc_code), None)\n",
    "\n",
    "    if not existing_record:\n",
    "        existing_record = {\n",
    "            \"soc_code\": soc_code,\n",
    "            \"title\": group.iloc[0][\"occ_title\"],\n",
    "            \"description\": description_map.get(soc_code, \"\"),\n",
    "            \"salary\": {},\n",
    "            \"education\": {key: \"\" for key in edu_map.values()},\n",
    "            \"typicalSkills\": []\n",
    "        }\n",
    "        records.append(existing_record)\n",
    "\n",
    "    year_str = str(year)\n",
    "    existing_record[\"salary\"].setdefault(year_str, {})\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        state = row[\"area_title\"]\n",
    "        industry = row[\"naics_title\"]\n",
    "        existing_record[\"salary\"][year_str].setdefault(state, {})[industry] = {\n",
    "            \"A_MEDIAN\": float(row[\"a_median\"]),\n",
    "            \"M_PCT10\": float(row[\"M_PCT10\"]),\n",
    "            \"M_MEDIAN\": float(row[\"M_MEDIAN\"]),\n",
    "            \"M_PCT90\": float(row[\"M_PCT90\"])\n",
    "        }\n",
    "\n",
    "# Add education data\n",
    "for record in records:\n",
    "    soc_code = record[\"soc_code\"]\n",
    "    edu_rows = education_df[(education_df[\"SOC\"] == soc_code) | (education_df[\"SOC\"] == \"00-0000\")]\n",
    "    for _, edu_row in edu_rows.iterrows():\n",
    "        est_code = edu_row[\"ESTIMATECODE\"]\n",
    "        est_value = str(edu_row[\"ESTIMATE\"])\n",
    "        if est_code in edu_map:\n",
    "            record[\"education\"][edu_map[est_code]] = est_value\n",
    "\n",
    "# Add skills data\n",
    "for record in records:\n",
    "    soc_code = record[\"soc_code\"]\n",
    "    skills_row = skills_df[skills_df[\"SOC_CODE\"] == soc_code]\n",
    "    if not skills_row.empty:\n",
    "        raw_skills = skills_row.iloc[0][\"TYPICAL_SKILLS\"].replace(\"'\", \"\\\"\")\n",
    "        try:\n",
    "            skills_list = json.loads(raw_skills)\n",
    "            record[\"typicalSkills\"] = sorted(set(skills_list))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"⚠️ Error decoding skills for SOC {soc_code}\")\n",
    "            record[\"typicalSkills\"] = []\n",
    "\n",
    "# Add timestamps\n",
    "timestamp = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "# Write to CSV (optional, for backup)\n",
    "output_file = \"dynamodb_ready_soc_output.csv\"\n",
    "with open(output_file, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\n",
    "        \"soc_code\", \"title\", \"description\", \"salary\", \"education\", \"typicalSkills\", \"createdAt\", \"updatedAt\"\n",
    "    ]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for rec in records:\n",
    "        writer.writerow({\n",
    "            \"soc_code\": rec[\"soc_code\"],\n",
    "            \"title\": rec[\"title\"],\n",
    "            \"description\": rec[\"description\"],\n",
    "            \"salary\": json.dumps(rec[\"salary\"], ensure_ascii=False),\n",
    "            \"education\": json.dumps(rec[\"education\"], ensure_ascii=False),\n",
    "            \"typicalSkills\": json.dumps(rec[\"typicalSkills\"], ensure_ascii=False),\n",
    "            \"createdAt\": timestamp,\n",
    "            \"updatedAt\": timestamp\n",
    "        })\n",
    "\n",
    "print(f\"✅ DynamoDB-ready CSV written to '{output_file}' with {len(records)} records.\")\n",
    "\n",
    "# Upload to DynamoDB\n",
    "def batch_write_to_dynamodb(table_name, items):\n",
    "    dynamodb = boto3.resource('dynamodb')\n",
    "    table = dynamodb.Table(table_name)\n",
    "\n",
    "    with table.batch_writer(overwrite_by_pkeys=[\"soc_code\"]) as batch:\n",
    "        for item in items:\n",
    "            try:\n",
    "                batch.put_item(Item=item)\n",
    "            except (BotoCoreError, ClientError) as e:\n",
    "                print(f\"❌ Failed to insert item {item.get('soc_code')}: {e}\")\n",
    "\n",
    "# Format items for DynamoDB insertion\n",
    "dynamodb_items = []\n",
    "for rec in records:\n",
    "    item = {\n",
    "        \"soc_code\": rec[\"soc_code\"],\n",
    "        \"title\": rec[\"title\"],\n",
    "        \"description\": rec[\"description\"],\n",
    "        \"salary\": json.dumps(rec[\"salary\"], ensure_ascii=False),\n",
    "        \"education\": json.dumps(rec[\"education\"], ensure_ascii=False),\n",
    "        \"typicalSkills\": json.dumps(rec[\"typicalSkills\"], ensure_ascii=False),\n",
    "        \"createdAt\": timestamp,\n",
    "        \"updatedAt\": timestamp\n",
    "    }\n",
    "    dynamodb_items.append(item)\n",
    "\n",
    "print(f\"☁️ Uploading {len(dynamodb_items)} records to DynamoDB table '{TABLE_NAME}'...\")\n",
    "batch_write_to_dynamodb(TABLE_NAME, dynamodb_items)\n",
    "print(f\"✅ Upload complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found salary files: ['filtered_occupation_data_2016.csv', 'filtered_occupation_data_2017.csv', 'filtered_occupation_data_2018.csv', 'filtered_occupation_data_2019.csv', 'filtered_occupation_data_2020.csv', 'filtered_occupation_data_2021.csv', 'filtered_occupation_data_2022.csv', 'filtered_occupation_data_2023.csv', 'filtered_occupation_data_2024.csv']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime, timezone\n",
    "import uuid\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError, BotoCoreError\n",
    "import time\n",
    "from decimal import Decimal\n",
    "\n",
    "# Insert your table names here\n",
    "CAREER_TABLE = 'your-careerData-table-name'\n",
    "SALARY_TABLE = 'your-salaryData-table-name'\n",
    "\n",
    "# Directory setup\n",
    "dataset_dir = \"Datasets/\"\n",
    "salary_dir = f\"{dataset_dir}Salaries/\"\n",
    "salary_files = sorted(glob(os.path.join(salary_dir, \"filtered_occupation_data_*.csv\")))\n",
    "\n",
    "if not salary_files:\n",
    "    raise FileNotFoundError(\"❌ No salary_data_*.csv files found.\")\n",
    "\n",
    "print(f\"✅ Found salary files: {[os.path.basename(f) for f in salary_files]}\")\n",
    "\n",
    "def safe_decimal(val):\n",
    "    try:\n",
    "        if val is None:\n",
    "            return None\n",
    "        dec = Decimal(str(val))\n",
    "        if math.isnan(float(dec)) or math.isinf(float(dec)):\n",
    "            return None\n",
    "        return dec\n",
    "    except (InvalidOperation, ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "# Load salary data\n",
    "salary_dfs = []\n",
    "for file in salary_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        df[\"YEAR\"] = int(file.split(\"_\")[-1].split(\".\")[0])\n",
    "        salary_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {file}: {e}\")\n",
    "\n",
    "salary_df = pd.concat(salary_dfs, ignore_index=True)\n",
    "\n",
    "# Load metadata\n",
    "education_df = pd.read_csv(os.path.join(dataset_dir, \"education_data.csv\"))\n",
    "skills_df = pd.read_csv(os.path.join(dataset_dir, \"skills_data.csv\"))\n",
    "description_df = pd.read_csv(os.path.join(dataset_dir, \"description.csv\"))\n",
    "description_df[\"Code\"] = description_df[\"Code\"].astype(str)\n",
    "description_map = description_df.set_index(\"Code\")[\"Description\"].to_dict()\n",
    "\n",
    "# Convert salary fields to numeric\n",
    "salary_df[\"a_pct10\"] = pd.to_numeric(salary_df[\"a_pct10\"], errors=\"coerce\")\n",
    "salary_df[\"a_median\"] = pd.to_numeric(salary_df[\"a_median\"], errors=\"coerce\")\n",
    "salary_df[\"a_pct90\"] = pd.to_numeric(salary_df[\"a_pct90\"], errors=\"coerce\")\n",
    "salary_df[\"M_PCT10\"] = (salary_df[\"a_pct10\"] / 12).round(2)\n",
    "salary_df[\"M_MEDIAN\"] = (salary_df[\"a_median\"] / 12).round(2)\n",
    "salary_df[\"M_PCT90\"] = (salary_df[\"a_pct90\"] / 12).round(2)\n",
    "\n",
    "# Education mapping\n",
    "edu_map = {\n",
    "    \"Less_than_hs\": \"LESS_THAN_HS\",\n",
    "    \"hs_or_eq\": \"HIGH_SCHOOL\",\n",
    "    \"Associate_degree\": \"ASSOCIATE\",\n",
    "    \"Bachelor_degree\": \"BACHELOR\",\n",
    "    \"Master_degree\": \"MASTERS\",\n",
    "    \"Doctorate_degree\": \"DOCTORATE\",\n",
    "    \"No_requirement\": \"NO_REQ\",\n",
    "    \"Professional_degree\": \"PROFESSIONAL\"\n",
    "}\n",
    "\n",
    "# Timestamps\n",
    "timestamp = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "# Generate salaryData records and build salary_map\n",
    "career_records = []\n",
    "salary_records = []\n",
    "salary_map_by_occ = {}\n",
    "\n",
    "for _, row in salary_df.iterrows():\n",
    "    occ_code = row[\"occ_code\"]\n",
    "    year = str(row[\"YEAR\"])\n",
    "    area = row[\"area_title\"]\n",
    "    industry = row[\"naics_title\"]\n",
    "\n",
    "    guid = str(uuid.uuid4())\n",
    "\n",
    "    # Create salary record\n",
    "    salary_records.append({\n",
    "        \"guid\": guid,\n",
    "        \"year\": year,\n",
    "        \"area\": area,\n",
    "        \"occupation\": occ_code,\n",
    "        \"industry\": industry,\n",
    "        \"annual_median\": safe_decimal(str(row[\"a_median\"])),\n",
    "        \"monthly_median\": safe_decimal(str(row[\"M_MEDIAN\"])),\n",
    "        \"monthly_pct10\": safe_decimal(str(row[\"M_PCT10\"])),\n",
    "        \"monthly_pct90\": safe_decimal(str(row[\"M_PCT90\"]))\n",
    "    })\n",
    "\n",
    "    # Build mapping for careerData\n",
    "    if occ_code not in salary_map_by_occ:\n",
    "        salary_map_by_occ[occ_code] = {}\n",
    "    if year not in salary_map_by_occ[occ_code]:\n",
    "        salary_map_by_occ[occ_code][year] = []\n",
    "    salary_map_by_occ[occ_code][year].append(guid)\n",
    "\n",
    "# Build careerData records\n",
    "unique_occ_codes = salary_df[\"occ_code\"].unique()\n",
    "\n",
    "for occ_code in unique_occ_codes:\n",
    "    title = salary_df[salary_df[\"occ_code\"] == occ_code].iloc[0][\"occ_title\"]\n",
    "    description = description_map.get(occ_code, \"\")\n",
    "\n",
    "    # Education\n",
    "    education = {val: \"\" for val in edu_map.values()}\n",
    "    edu_rows = education_df[(education_df[\"SOC\"] == occ_code) | (education_df[\"SOC\"] == \"00-0000\")]\n",
    "    for _, edu_row in edu_rows.iterrows():\n",
    "        est_code = edu_row[\"ESTIMATECODE\"]\n",
    "        if est_code in edu_map:\n",
    "            education[edu_map[est_code]] = str(edu_row[\"ESTIMATE\"])\n",
    "\n",
    "    # Skills\n",
    "    skills_row = skills_df[skills_df[\"SOC_CODE\"] == occ_code]\n",
    "    typicalSkills = []\n",
    "    if not skills_row.empty:\n",
    "        raw_skills = skills_row.iloc[0][\"TYPICAL_SKILLS\"].replace(\"'\", \"\\\"\")\n",
    "        try:\n",
    "            typicalSkills = sorted(set(json.loads(raw_skills)))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"⚠️ Error decoding skills for SOC {occ_code}\")\n",
    "\n",
    "    # Final record\n",
    "    career_records.append({\n",
    "        \"occ_code\": occ_code,\n",
    "        \"occ_title\": title,\n",
    "        \"description\": description,\n",
    "        \"salary\": salary_map_by_occ.get(occ_code, {}),\n",
    "        \"education\": education,\n",
    "        \"skills\": typicalSkills\n",
    "    })\n",
    "\n",
    "# Optional backup to CSV\n",
    "with open(\"careerData_output.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        \"occ_code\", \"occ_title\", \"description\", \"salary\", \"education\", \"skills\"\n",
    "    ])\n",
    "    writer.writeheader()\n",
    "    for rec in career_records:\n",
    "        writer.writerow({\n",
    "            \"occ_code\": rec[\"occ_code\"],\n",
    "            \"occ_title\": rec[\"occ_title\"],\n",
    "            \"description\": rec[\"description\"],\n",
    "            \"salary\": json.dumps(rec[\"salary\"]),\n",
    "            \"education\": json.dumps(rec[\"education\"]),\n",
    "            \"skills\": json.dumps(rec[\"skills\"])\n",
    "        })\n",
    "\n",
    "with open(\"salaryData_output.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        \"guid\", \"year\", \"area\", \"occupation\", \"industry\",\n",
    "        \"annual_median\", \"monthly_median\", \"monthly_pct10\", \"monthly_pct90\"\n",
    "    ])\n",
    "    writer.writeheader()\n",
    "    for rec in salary_records:\n",
    "        writer.writerow(rec)\n",
    "\n",
    "print(\"✅ Backup files written: 'careerData_output.csv', 'salaryData_output.csv'\")\n",
    "\n",
    "# Upload function\n",
    "def batch_write(table_name, items, key_field, max_retries=3):\n",
    "    dynamodb = boto3.resource('dynamodb')\n",
    "    table = dynamodb.Table(table_name)\n",
    "    failed_items = []\n",
    "\n",
    "    with table.batch_writer(overwrite_by_pkeys=[key_field]) as batch:\n",
    "        for item in items:\n",
    "            retries = 0\n",
    "            while retries < max_retries:\n",
    "                try:\n",
    "                    batch.put_item(Item=item)\n",
    "                    break\n",
    "                except (ClientError, BotoCoreError) as e:\n",
    "                    retries += 1\n",
    "                    print(f\"❌ Retry {retries}/{max_retries} for item {item.get(key_field)} — {e}\")\n",
    "                    time.sleep(1)\n",
    "            else:\n",
    "                failed_items.append(item.get(key_field))\n",
    "\n",
    "    if failed_items:\n",
    "        print(f\"⚠️ Failed to upload {len(failed_items)} items: {failed_items}\")\n",
    "    else:\n",
    "        print(f\"✅ All items uploaded successfully to {table_name}\")\n",
    "\n",
    "# Upload salaryData\n",
    "print(f\"☁️ Uploading {len(salary_records)} salaryData items to DynamoDB...\")\n",
    "batch_write(SALARY_TABLE, salary_records, key_field=\"guid\")\n",
    "\n",
    "# Upload careerData\n",
    "print(f\"☁️ Uploading {len(career_records)} careerData items to DynamoDB...\")\n",
    "batch_write(CAREER_TABLE, career_records, key_field=\"occ_code\")\n",
    "\n",
    "print(\"🎉 Done! All records processed and uploaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
